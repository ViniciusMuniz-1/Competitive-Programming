#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Capítulo 1:

Entrada e saída:

    Os seguintes comandos devem ser colocados no início do código para um melhor desempenho de entrada e saída:
    ios::sync_with_stdio(0);
    cin.tie(0);

    "\n" é mais rápido do que o endl, porque o endl sempre causa uma operação de limpeza (flush).

    Os comandos scanf e printf do C funcionam no C++, são mais rápidos, mas também mais complicados de se usar.

    Código para conseguir pegar uma linha inteira separada por espaços:
    string s;
    getline(cin, s);

    Se a quantidade de entradas for desconhecida, pode-se usar:
    while (cin >> x) {
    // code
    }

    Algumas competições utilizam arquivos, deixarei o código que deve ser colocado no início do código para esses casos:
    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Números flutuantes:

    Para números flutuantes, temos dois tipos de variáveis em C++, double (64-bit) e long double (80-bit)

    Pode ser arriscado comparar números flutuantes devido os erros de precisão, então pode ser melhor assumir que dois números são iguais se a subtração entre eles for menor que um número muito pequeno, como 10^-9. Como exemplo:
    if (abs(a-b) < 1e-9) {
    // a and b are equal
    }

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Dicas para diminuir o código:

    Usar typedef para diminuir o nome de variáveis, por exemplo:
    typedef long long ll; typedef vector<int> vi; typedef pair<int,int> pi;

->Macros:

    Outra maneira de diminuir os códigos é definir macros. Um macro significa que certas linhas no código serão alteradas antes da compilação. Exemplos:
    #define F first
    #define S second
    #define PB push_back()
    #define MP make_pair()

    Um macro também pode ter parâmetros, o que faz ser possível diminuir loops e outras estruturas. Exemplo:
    #define REP(i,a,b) for (int i = a; i <= b; i++)

    Esse código:
    for (int i = 1; i <= n; i++) {
    search(i);
    }

    Vira isso:
    REP(i,1,n) {
    search(i);
    }

    Algumas vezes, macros causam bugs que são difíceis de detectar. Por exemplo, considere o código abaixo que calcula o quadrado de um número:
    #define SQ(a) a*a

    Esse código:
    cout << SQ(3+3) << "\n";

    Corresponde a isso:
    cout << 3+3*3+3 << "\n"; // 15

    Uma melhor versão desse macro seria:
    #define SQ(a) (a)*(a)

    Agora, o código:
    cout << SQ(3+3) << "\n";

    Corresponde a:
    cout << (3+3)*(3+3) << "\n" // 36

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Matemática:

->Soma:
    PA: n(a+b)/2 onde a é o primeiro número, b é o último número e n a quantidade de números da sequência
    PG: bk-a/k-1 onde b é o último número, a é o primeiro número e k é a razão entre os números

->Teoria dos conjuntos:
    Um conjunto é uma coleção de elementos, como: X = {5, 8, 9, 10}

    |S| denota o tamanho do conjunto, ou seja a quantidade de elementos dentro do conjunto.

    Por exemplo, |X| = 4

    Se S possui um elemento x, dizemos que x ∈ s, e caso não, x ∉ S.
    5 ∈ X e 7 ∉ X.

    Novos conjuntos podem ser criados com as seguintes operações:
        ->Interseção; União; Complemento; Diferença.

Se cada elemento do conjunto A também pertence ao conjunto S, dizemos que A é um subconjunto de S.Alguns conjuntos frequentemente usados são N (números naturais), Z (números inteiros), Q (números racionais) e R (números reais).

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Lógica:
    ->O valor de uma expressão lógica é verdadeiro (1) ou falso (0). Os operadores lógicos mais importantes são: ¬ (negação), ∧ (conjunção), ∨ (disjunção), ⇒ (implicação) e ⇔ (equivalência).
    ->Predicado é uma expressão que é verdadeira ou falsa dependendo dos parâmetros. Os predicados geralmente são representados por letras maiúsculas. Por exemplo, podemos definir um predicado P(x) que é verdadeiro exatamente quando x é um número primo. Usando essa definição, P(7) é verdadeiro, mas P(8) é falso.
    ->Um quantificador conecta uma expressão lógica aos elementos de um conjunto. Os quantificadores mais importantes são ∀ (para todo) e ∃ (existe).

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 2:

->Complexidade de tempo: 
    A eficiência dos algoritmos é muito importante em uma competição de programação. É fácil implementar uma solução que resolve os problemas devagar, mas o verdadeiro desafio está em implementar algoritmos rápidos.
    A complexidade de tempo estima quanto tempo um algoritmo vai usar para cada entrada. A ideia é representar a eficiência de uma função que tem como parâmetro o tamanho da entrada. Calculando a complexidade de tempo, podemos descobrir se o algoritmo é rápido o suficiente para ser implementado.

    Regras do cálculo:
        A complexidade de tempo dos algoritmos é representada por O(...) em que o 3 pontos representam alguma função. Geralmente, a variável n denota o tamanho da entrada. Por exemplo, se a entrada for um array de números, n será o tamanho do array, e se a entrada for uma string, n será o comprimento da string.

        Laços:
            Uma razão comum do motivo de um algoritmo ser lento é por conter muitos laços que dependem do tamanho de entrada. Quanto mais loops aninhados o algoritmo contém, mais lento ele se torna. Se houver k loops aninhados, a complexidade de tempo é O(n^k).
            Por exemplo, a complexidade do código abaixo é O(n):
                for(int i = 0; i <= n; ++i){
                    //code
                }
            E esse código possui complexidade de O(n²):
                for(int i = 0; i < n; ++i){
                    for(int j = 0; j < n; ++j){
                        // code
                    }
                }
        
        Ordem de grandeza:
            A complexidade de tempo não diz exatamente a quantidade de vezes que o código dentro do laço será executado, apenas mostra a ordem de grandeza. No exemplo a seguir, o código dentro do loop será executado 3n, n+5 e ~n/2 vezes, mas a complexidade de cada código é O(n).
            for(int i = 0; i < 3*n; ++i){
                //code
            }

            for(int i = 0; i < n+5; ++i){
                //code
            }

            for(int i = 0; i < n; i+=2){
                //code
            }

        Fases:
            Se um algoritmo consiste em consecutivas fases, o tempo total de complexidade será o maior tempo de complexidade de uma única fase. A razão disso é que a fase mais lenta será o gargalo do código.
            Para exemplo, o código seguinte consiste em 3 fases que tem como complexidade de tempo, respectivamente, O(n), O(n²) e O(n). Então, a complexidade total será O(n²)!
            for(int i = 0; i < n; ++i){
                //code
            }

            for(int i = 0; i < n; ++i){
                for(int j = 0; j < n; ++j){
                    //code
                }
            }

            for(int i = 0; i < n; ++i){
                //code
            }

        Várias variáveis:
            De vez em quando, a complexidade de tempo depende de vários fatores. Nesse caso, a fórmula da complexidade de tempo conterá várias variáveis.
            Para exemplo, a complexidade do código abaixo é O(n*m):
                for(int i = 0; i < n; ++i){
                    for(int j = 0; j < m; ++j){
                        //code
                    }
                }

        Recursividade:
            A complexidade de tempo de uma função recursiva depende do número de vezes que a função é chamada e da complexidade de tempo de uma única chamada. A complexidade de tempo total é o produto desses valores.
            Para exemplo, considere o código abaixo:
                void f (int n){
                    if(n==1) return;
                    f(n-1);
                }
                
            A chamada f(n) causa n chamadas de função, e a complexidade de cada chamada é O(1). Então, a complexidade de tempo será O(n).
            
            Como outro exemplo, temos a seguinte função:
                void g(int n){
                    if(n == 1) return;
                    g(n-1);
                    g(n-1);
                }

            Nesse caso, cada chamada de função gera outras duas chamadas, exceto para n=1. Vamos ver o que acontece quando g é chamado com um parâmetro n. O seguinte esquema nos mostra a chamada de função produzida por uma chamada:

            Chamada de função  | Número de chamadas
            g(n)                                  1
            g(n-1)                                2
            g(n-2)                                4
            ...                                 ...
            g(1)                                2^n-1

            Baseado nisso, a complexidade de tempo será:
                1+2+4+...+2^n-1 = 2^n - 1 = O(2^n).

        Classes de complexidade de tempo:

            O(1) - Complexidade constante: O tempo de execução do algoritmo não depende do tamanho da entrada; é sempre constante;
            O(log n) - Complexidade logarítmica: O tempo de execução do algoritmo cresce de forma logarítmica em relação ao tamanho da entrada;
            O(square(n)) - Um algoritmo de raiz quadrada é mais lento que O(log n) mas mais rápido que O(n). 
            O(n) - Complexidade linear: O tempo de execução do algoritmo cresce linearmente em relação ao tamanho da entrada;
            O(n²) - Complexidade quadrática: O tempo de execução do algoritmo cresce quadraticamente em relação ao tamanho da entrada;
            O(n³) - Complexidade cúbica: O tempo de execução do algoritmo cresce cubicamente em relação ao tamanho da entrada;
            O(nlogn) - Complexidade log-linear: O tempo de execução do algoritmo cresce de forma log-linear em relação ao tamanho da entrada.
            O(2^n) - Essa complexidade de tempo frequentemente indica que o algoritmo itera por todos os subconjuntos dos elementos de entrada. Por exemplo, os subconjuntos de {1,2,3} são vazios, {1}, {2}, {3}, {1,2}, {1,3}, {2,3} e {1,2,3}.
            O(n!) - Essa complexidade de tempo frequentemente indica que o algoritmo itera por todas as permutações dos elementos de entrada. Por exemplo, as permutações de {1,2,3} são (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2) e (3,2,1).

            Um algoritmo é polinomial se sua complexidade de tempo for no máximo O(n^k), onde k é uma constante. Todas as complexidades de tempo acima, exceto O(2^n) e O(n!), são polinomiais. Na prática, a constante k geralmente é pequena, e, portanto, uma complexidade de tempo polinomial significa aproximadamente que o algoritmo é eficiente. 
            A maioria dos algoritmos neste livro é polinomial. Ainda assim, existem muitos problemas importantes para os quais nenhum algoritmo polinomial é conhecido, ou seja, ninguém sabe como resolvê-los de forma eficiente. Problemas NP-difíceis são um conjunto importante de problemas para os quais nenhum algoritmo polinomial é conhecido.
            
        Estimando eficiência:

            Calculando a complexidade de tempo de um algoritmo, é possível checar, antes da implementação, se ele é eficiente o suficiente para resolver o problema. O ponto de partida para estimativas é o fato que os computadores modernos podem executar algumas centenas de milhões de números de operação em segundos.
            Para exemplo, vamos assumir que o tempo limite para um problema seja de 1 segundo e o tamanho de entrada seja n = 10^5. Se a complexidade de tempo for O(n²), o algoritmo iria performar cerca de (10^5)² = 10^10 operações. Isso deve demandar um pouco mais que 10 segundos, então o algoritmo parece ser lento demais para resolver o problema.
            Por outro lado, dado o tamanho de entrada, nós podemos tentar adivinhar o tempo de complexidade que o algoritmo deve ter para resolver o problema. A tabela a seguir contém algumas estimativas de tempo assumindo o tempo limite de 1 segundo.
            
            Tamanho da entrada | Tempo de complexidade requirido
            n <= 10                                        O(n!)
            n <= 20                                        O(2^n)
            n <= 500                                       O(n³)
            n <= 5000                                      O(n²)
            n <= 10^6                         O(nlogn) or O(n)
            n é maior                           O(1) ou O(log n)

            Para exemplo, se o tamanho da entrada for n = 10^5, é provável que a complexidade de tempo esperado seja de O(n) ou O(log n). Essa informação faz com que seja mais fácil projetar o algoritmo, pois descarta abordagens que resultariam em um algoritmo com uma complexidade de tempo pior.
            Mesmo assim, é importante lembrar que o tempo de complexidade é apenas uma estimativa da eficiência, pois pode esconder fatores constantes. Por exemplo, um algoritmo que rode em tempo O(n) pode performar n/2 ou 5n operações. 

        Soma máxima de um subconjunto:

            Frequentemente existem diversos algoritmos para resolver um problema, de forma que suas complexidades de tempo sejam diferentes. Esse seção discute um problema clássico que possui uma solução direta em O(n³). Porém, projetando um melhor algoritmo, é possível resolver o problema em O(n²) ou até mesmo em O(n).
            
            Problema: dado um array de n números, nossa missão é calcular a soma máxima de um subconjunto, ou seja, a maior soma possível de uma sequência de valores consecutivos em um array. O problema se torna interessante quando podem haver valores negativos no array. Por exemplo, no array:
            [-1, 2, 4, -3, 5, 2, -5, 2]

            O subarray seguinte produz uma soma máxima de 10:
            [-1, 2, 4, -3, 5, 2, -5, 2]
            2+4+(-3)+5+2 = 10

            Assumimos que um array vazio é permitido, então a soma máxima de um subarray sempre será, pelo menos, 0.

            Algoritmo 1:
                Uma solução direta para resolver o problema é percorrer todos os subarray possíveis, calcular a soma dos valores em cada subarray e manter a soma máxima. O seguinte código implementa esse algoritmo:

                int best = 0;
                for(int a = 0; a < n; a++){
                    for(int b = a; b < n; b++){
                        int sum = 0;
                        for(int k = a; k <= b; k++){
                            sum += array[k];
                        }
                        best = max(best, sum);
                    }
                }
                cout << best << "\n";

                As variáveis a e b fixam o primeiro e o último index do subarray, e a soma dos valores é calculada e armazenada na variável sum. A variável best contém a soma máximo encontrada durante a procura.
                A complexidade de tempo desse código é O(n³), pois consiste em 3 array aninhados que dependem do tamanho de entrada.

            Algoritmo 2:
                É facíl tornar o algoritmo 1 mais eficiente removendo um laço dele. Isso é possível calculando a soma ao mesmo tempo em que a extremidade direita do subarray se move. O resultado é o código abaixo:
                
                int best = 0;
                for(int a  = 0; a <n; a++){
                    int sum = 0;
                    for(int b = a; b < n; b++){
                        sum+= array[b];
                        best = max(best, sum);
                    }
                }
                cout << best << "\n";

            Algoritmo 3:
                Surpreendentemente, é possível resolver esse problema em um tempo O(n), o que significa que um único laço é o suficiente. A ideia é calcular, para cada uma das posições do array, a soma máxima de um subarrray que termina nessa posição. Após isso, a resposta para o problema será o máximo entre essas somas.
                Considere o sub problema de encontrar a soma máxima de um subarray que termina na posição k. Existem 2 possibilidades:
                1 - O subarray apenas contém o elemento na posição k
                2 - O subarray consiste em um subarray que termina na posição k -1, seguido pelo elemento na posição k.
                No segundo caso, uma vez que queremos encontrar um subarray com soma máxima, o subarray que termina na posição k-1 deve ter a soma máxima. Assim, podemos resolver o problema de forma eficiente calculando a soma máxima do subarray para cada posição final da esquerda para a direita.
                O seguinte código implementa esse algoritmo:

                int best = 0, sum = 0;
                for(int k = 0; k < n; k++){
                    sum = max(array[k], sum+array[k]);
                    best = max(best, sum);
                }
                cout << best << "\n";

                O algoritmo contém apenas um laço que depende do tamanho da entrada, então a complexidade de tempo é O(n). Esse é o melhor tempo possível, pois qualquer outro algoritmo para o problema examina cada elemento do array pelo menos uma vez.
            
            Comparação de eficiência:
                É interessante estudar o quão eficiente são os algoritmos na prática. A seguinte tabela mostra o tempo de execução os algoritmos acima para diferentes valores de n em um computador moderno.
                Para cada teste, a entrada foi gerada aleatoriamente. O tempo necessário para a leitura da entrada não foi mensurado.
                
                array de tamanho n  | Algoritmo 1 | Algoritmo 2 | Algoritmo 3
                                10²          0.0s          0.0s         0.0s
                                10³          0.1s          0.0s         0.0s
                                10^4      > 10.0s          0.0s         0.0s
                                10^5      > 10.0s          0.1s         0.0s
                                10^6      > 10.0s          5.3s         0.0s
                                10^7      > 10.0s       > 10.0s         0.0s
                                10^8      > 10.0s       > 10.0s         0.0s

                A comparação mostra que os algoritmos são eficientes quando o tamanho de entrada é pequeno, mas entradas maiores trazem a tona a diferença entre o tempo de execução dos algoritmos. Algoritmo 1 se torna mais lento quando n = 10^4, e o Algoritmo 2 se torna mais lento quando n = 10^5. Apenas o algoritmo 3 é hábil à processar até mesmo as maiores entradas instantaneamente.
            
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 3:

->Ordenação:
    Ordenação é um problema fundamental de design de algoritmo. Vários algoritmos eficientes usam ordenação como uma sub-rotina, porque é frequentemente mais fácil processar dados se os elementos estiverem ordenados.
    Por exemplo, o problema "um array possui dois elementos iguais?" é fácil de resolver usando ordenação. Se o array possuir dois elementos iguais, eles estarão do lado um do outro depois da ordenação, então é fácil encontrá-los. Também, o problema "qual o elemento mais frequente no array?" pode ser resolvido de forma similar.
    Existem vários algoritmos para ordenação, e eles são bons exemplos de como aplicar diferentes técnicas de design de algoritmos. Os algoritmos gerais de ordenação eficientes atuam em tempo O(nlogn), e vários algoritmos que usam ordenação como sub-rotina também tem essa complexidade de tempo.

    Teoria da ordenação:
        O problema básico na ordenação é o seguinte:
        Dado um array que contém n elementos, sua tarefa é ordenar os elementos em ordem crescente.

        Por exemplo, o array:
        [1, 3, 8, 2, 9, 2, 5, 6]

        Deve estar assim após a ordenação:
        [1, 2, 2, 3, 5, 6, 8, 9]

    Algoritmos em O(n²):
        Algoritmos simples de ordenação de array funcionam em tempo O(n²). Esses algoritmos são curtos e frequentemente consistem em 2 laços aninhados. Um famoso algoritmo de ordenação com tempo O(n²) é o bubble sort, no qual os elementos "borbulham" no array de acordo com seus valores.
        O bubble sort consiste em n fases. Em cada fase, o algoritmo percorre os elementos do array. Sempre que 2 elementos consecutivos não estiverem na ordem correta, o algoritmo troca eles de posição. O algoritmo pode ser implementado com o código abaixo:

        for(int i = 0; i < n; ++i){
            for(int j = 0; j < n-1; ++j){
                if(array[j] > array[j+1]){
                    swap(array[j], array[j+1]);
                }
            }
        }

        Após a primeira fase do algoritmo, o maior valor estará na posição correta, e geralmente, após k fases, os k maiores elementos estarão nas posições corretas. Assim, após n fases, todo o array está ordenado.

        Por exemplo, no array:
        [1, 3, 8, 2, 9, 2, 5, 6]

        a primeira fase do bubble sort mudará os seguintes da seguinte forma:
        [1, 3, 2, 8, 9, 2, 5, 6]

        [1, 3, 2, 8, 2, 9, 5, 6]

        [1, 3, 2, 8, 2, 5, 9, 6]

        [1, 3, 2, 8, 2, 5, 6, 9]

    Inversões:
        Bubble sort é um exemplo de um algoritmo de ordenação que sempre troca elementos consecutivos no array. Isso torna o tempo de complexidade de um algoritmo como esse sempre ser, pelo menos, O(n²), porque no pior caso, O(n²) trocas são requeridas para ordenar o array.
        Um conceito útil ao analisar algoritmos de ordenação é a inversão: um par de elementos do array {array[a], array[b]} tal que a < b and array[a] > array[b], ou seja, esses elementos estão na ordem errada. Por exemplo, o array:
        [1, 2, 2, 6, 3, 5, 9, 8]

        possui 3 inversões: (6, 3), (6, 5) e (9, 8). O número de inversões indica o quanto de trabalho se terá para ordenar um array. Um array está completamente ordenado quando não há inversões. Por outro lado, se os elementos do array estão em ordem inversa, o número de inversões é o maior possível: 

        1 + 2 + ... + (n-1) = (n(n-1))/2 = O(n²)

        Trocar um par de elementos consecutivos que estão na ordem errada remove exatamente uma inversão do array. Portanto, se o algoritmo de ordenação só pode trocar elementos consecutivos, cada troca remove pelo menos uma inversão, e o tempo de complexidade do algoritmo se torna ao menos O(n²).

    Algoritmo em tempo O(nlogn):
        É possível ordenar um array de maneira eficiente em tempo O(nlogn) usando algoritmos que não se limitam à troca de elementos consecutivos. Um algoritmo desse tipo é o merge sort, qual se baseia em recursividade.
        Merge sort ordena um subarray array[a...b] da seguinte forma:
        1. Se a = b, não faça nada, porque o subarray já está ordenado.
        2. Calcule a posição do elemento do meio: k = ~[a+b/2].
        3. Recursivamente ordene o subarray array[a...k].
        4. Recursivamente ordene o subarray array[k+1...b].
        5. Junte os subarrays ordenados array[a...k] e array[k+1...b] em um subarray ordenado array[a...b].

        O merge sort é um algoritmo eficiente, pois reduz pela metade o tamanho do subarray em cada etapa. A recursão consiste em O(log n) níveis, e processar cada um dos níveis leva tempo O(n). Juntar os subarrays array[a...k] e array[k+1...b] é possível de ser feito em tempo linear O(1), porque eles já estão ordenado.
        Por exemplo, considere ordenar o seguinte array:
        [1, 3, 6, 2, 8, 2, 5, 9]
        
        O array será divido em 2 subarrayas da seguinte forma:
        [1, 3, 6, 2] [8, 2, 5, 9]

        Então, os subarrays serão ordenados recursivamente da seguinte forma:
        [1, 2, 3, 6] [2, 5, 8, 9]

        Finalmente, o algoritmo junta os dois subarrays ordenados e cria um último array ordenado:
        [1, 2, 2, 3, 5, 6, 8, 9]

    Limite inferior de ordenação:
        É possível ordenar um array mais rápido do que em tempo O(nlogn)? Acontece que isso não é possível quando nos restringimos a algoritmos de ordenação baseados em comparar os elementos do array.
        O limite inferior para a complexidade de tempo pode ser provado considerando a ordenação como um processo em que cada comparação de dois elementos fornece mais informações sobre o conteúdo do array. O processo cria a seguinte árvore:

                x < y?
            x<y?       x<y?
        x<y?   x<y? x<y?  x<y?

        Aqui, "x<y?" significa que vários elementos x e y estão sendo comparados. Se x<y, o processo continua para a esquerda, caso contrário, para a direita. O resultado do processo são os possíveis caminhos para se ordenar o array, um total de n! formas. Por essa razão, a altura da árvore deve ser no mínimo
            log2(n!) = log2(1) + log2(2) + ... + log2(n)
        Obtemos um limite inferior para essa soma escolhendo os últimos n/2 elementos e alterando o valor de cada elemento para log2(n/2). Isso nos fornece uma estimativa
            log2(n!) >= (n/2) * log2(n/2)
        então a altura da árvore e o mínimo de etas possíveis para um algoritmos de ordenação no pior caso é nlogn.

    Counting sort:
        O limite inferior de nlogn não se aplica a algoritmos que não comparam elementos do array, mas usam outras informações. Como exemplo temos o algoritmo counting sort, que ordena um array em tempo O(n) assumindo que cada um dos elementos do array é um inteiro entre 0...c e c = O(n).
        O algoritmo cria um array de registro, cujos índices são elementos do array original. O algoritmo percorre o array original e calcula quantas vezes cada elemento aparece no array.
        Por exemplo, o array:
        [1, 3, 6, 9, 9, 3, 5, 9]

        corresponde ao seguinte array de registro:
         1  2  3  4  5  6  7  8  9
        [1, 0, 2, 0, 1, 1, 0, 0, 3]

        Por exemplo, o valor na posição 3 no array de registro é 2, pois o elemento 3 aparece 2 vezes no array original.
        A construção do array de registra leva tempo O(n). Depois disso, o array ordenado pode ser criado em tempo O(n) porque o número de ocorrências para cada elemento pode ser retirada do array de registro. Assim, o tempo de complexidade total do counting sort é O(n).
        Counting sort é um algoritmo eficiente, mas só pode ser usado quando a constante c é pequena o suficiente para que os elementos do array possam ser usados como índices no array de registro.

    Ordenando em C++:
        Quase sempre é uma má ideia usar algoritmos de ordenação feitos em casa em uma competição, pois existem boas implementações disponíveis nas linguagens de programação. Por exemplo, a biblioteca padrão do C++ contém a função sort que pode ser facilmente usada para ordenar arrays e outras estruturas de dados.
        Existem muitos benefícios em usar funções de bibliotecas. Primeiro, salva tempo porque não é necessário implementar uma outra função. Segundo, a implementação da biblioteca é com certeza correta e eficiente: não é provável que uma função de ordenar feita em casa seja melhor.
        Nessa seção, veremos como usar a função sort do C++. O seguinte código ordena um vetor em ordem crescente:

        vector<int> v = {4,2,5,3,5,8,3};
        sort(v.begin(), v.end());

        Após a ordenação, o conteúdo do vetor será [2,3,3,4,5,5,8]. O padrão de ordenação é crescente, mas a ordem inversa é possível com o código a seguir: 

        sort(v.rbegin(), v.rend());

        Um array comum pode ser ordenado da seguinte forma:
        
        int n = 7; //tamanho do array
        int a[] = {4,2,5,3,5,8,3};
        sort(a,a+n);

        O código seguinte ordena uma string s:

        string s = "monkey";
        sort(s.begin(), s.end());

        Ordenar uma string significa que os caracteres da string são ordenados. Por exemplo, a string "monkey" se torna "ekmnoy".

    Operadores de comparação:
        A função sort requer que um operador de comparação seja definido para o tipo de dados dos elementos a serem ordenados. Ao ordenar, esse operador será usado sempre que for necessário determinar a ordem de dois elementos.
        A maior parte dos tipos de dado do C++ possuem um operador de comparação embutido, então os elementos desse tipo podem ser ordenados automaticamente. Por exemplo, os números são ordenados de acordo com seus valores e strings são ordenadas em ordem alfabética.
        Pares(pair) são ordenados principalmente de acordo com seus primeiros elementos(first). No entanto, se os primeiros elementos de dois pares forem iguais, eles são ordenados de acordo com seus segundos elementos(second):
        
        vector<pair<int,int>> v;
        v.push_back({1, 5});
        v.push_back({2, 3});
        v.push_back({1, 2});
        sort(v.begin(), v.end());

        Após isso, a ordem dos pares será (1,2), (1,5) e (2,3).
        
        De uma maneira similar, tuplas (tuple) são ordenadas primeiramente pelo seu primeiro elemento, secundariamente pelo segundo elemento, etc.:
        
        vector<tuple<int,int,int>> v;
        v.push_back({2,1,4});
        v.push_back({1,5,3});
        v.push_back({2,1,3});
        sort(v.begin(), v.end());

        Após isso, a ordem das tuplas será (1,5,3),(2,1,3) e (2,1,4).

    Estruturas definidas pelo usuário:
        Estruturas definidas pelo usuário não possuem um operado de comparação automaticamente. O operador deve ser definido dentro do struct como uma função operator<, cujo parâmetro é outro elemento do mesmo tipo. O operador deve retornar true se o elemento for menor que o parâmetro, e falso se for maior.
        Por exemplo, a seguinte estrutura P contém as coordenadas x e y de um ponto. O operador de comparação é definido então os pontos são ordenados primeiramente pela coordenada x e secundariamente pela coordenada y.

        struct P{
            int x, y;
            bool operator<(const P & p){
                if (x != p.x) return x < p.x;
                else return y < p.y;
            }
        };

    Funções de comparação:
        Também é possível oferecer uma função de comparação externa para função sorte como uma função de retorno de chamada (callback function). Por exemplo, a seguinte função de comparação comp ordena strings primeiramente pelo tamanho e secundariamente pela ordem alfabética:

        bool comp(string a, string b) {
            if(a.size() != b.size()) return a.size() < b.size();
            return a < b;
        }

        Agora, um vetor de strings pode ser ordenado dessa forma:

        sort(v.begin(), v.end(), comp);

    Busca binária:
        Um método padrão para procurar um elemento em um array é usar um laço for que percorre os elementos do array. Por exemplo, o seguinte código procura se um elemento x está no array:

        for(int i = 0; i < n; ++i){
            if(x == array[i]){
                // x encontrado no index i
            }
        }

        A complexidade de tempo dessa abordagem é O(n), porque no pior caso é necessário checar todos os elementos do array. Se a ordem dos elementos for arbitrária, então essa é a melhor abordagem possível, pois não há informações adicionais disponíveis sobre onde no array devemos procurar pelo elemento x.
        Por outro lado, se o array estiver ordenado, a situação é diferente. Nesse caso, é possível performar a procura de maneira muito mais rápida, porque oa ordem dos elementos no array guia a procura. Os seguintes algoritmos de busca binária procuram eficientemente por um elemento em um array ordenado em um tempo de O(logn).

        ->Método 1:
            A maneira usual de se implementar a busca binária se assemelha a procurar uma palavra em um dicionário. A procura mantém uma região ativa no array, que inicialmente contém todos os elementos do array. Em seguida, um certo número de passos é realizado, cada um dos quais reduz pela metade o tamanho da região.
            Em cada etapa, a busca verifica o elemento do meio da região ativa. Se o elemento do meio for o elemento procurado, então a busca termina. Caso contrário, a busca continua recursivamente na metade esquerda ou direita da região, dependendo do valor do elemento do meio.
            A ideia acima pode ser implementada da seguinte maneira:

            int a  = 0, b = n-1;
            while(a <= b){
                int k = (a+b)/2;
                if(array[k] == x){
                    //x encontrado no índice k
                }
                if(array[k] > b) b = k-1;
                else a = k+1;
            }
            
            Nessa implementação, a região ativa é a...b, e inicialmente a região é 0...n-1. O algoritmo reduz pela metade o tamanho da região a cada etapa, então a complexidade de tempo é O(logn).

        ->Método 2:
            Um método alternativo de implementa a busca binário é baseado em uma maneira eficiente de percorrer os elementos do array. A ideia é fazer saltos e diminuir a velocidade quando se está mais perto do elemento-alvo.
            A busca percorre o array da esquerda para a direita, e o comprimento inicial do salto é n/2. Em cada etapa, o comprimento do salto será dividido pela metade: primeiro n/4, depois n/8, n/16, etc., até que finalmente o comprimento seja 1. Após os saltos, ou o elemento-alvo foi encontrado, ou sabemos que ele não aparece no array.
            O seguinte código implementa a ideia acima:

            int k = 0;
            for(int b = n/2; b >= 1; b/=2){
                while(k+b < n && array[k+b]<=x) k+=b;
            }
            if(array[k] == x){
                // x encontrado no índice k
            }

            Durante a busca, a variável b contém o tamanho do salto atual. A complexidade de tempo do algoritmo é O(logn), pois o código no laço while é executado no máximo duas vezes para cada comprimento de salto.

    Funções do C++:
        A biblioteca padrão do C++ contém as seguintes funções que são baseadas em busca binária e funcionam em tempo logarítmico:
        ->lower_bound retorna um ponteiro para o primeiro elemento cujo valor é pelo menos x.
        ->upper_bound retorna um ponteiro para o primeiro elemento cujo valor seja maior que x.
        ->equal_range retorna os dois ponteiros acima.

        As funções assumem que o array está ordenado. Se não houver tal elemento, o ponteiro aponta para o elemento após o último elemento do array. Por exemplo, o seguinte código determina se um array contém um elemento com valor x:

        auto k = lower_bound(array, array+n, x) - array;
        if(k < n && array[k]==x){
            //x encontrado no índice k
        }
        
        Então, o código a seguir conta o número de elementos cujo valor é x:
        
        auto a = lower_bound(array, array+n, x);
        auto b = upper_bound(array, array+n, x);
        cout << b-a << "\n";

        Usando equal_range, o código se torna menor:

        auto r = equal_range(array, array+n, x);
        cout << r.second-r.first << "\n";

    Encontrando a menor solução:
        Um importante uso da busca binária é encontrar a posição onde o valor de uma função muda. Suponha que desejamos encontrar o menor valor k que é uma solução válida para um problema. Nós temos uma função "ok(x)" que retorna verdadeiro se x for uma solução válida e falso caso contrário. Além disso, sabemos que "ok(x)" é falso quando x < k e verdadeiro quando x >= k. A situação é a seguinte:

        x     | 0       1   ...   k-1      k    k+1   ...
        ok(x) |false false  ... false   true   true   ...

        Agora, o valor de k pode ser encontrado usando busca binária:

        int x = -1;
        for(int b = z; b >= 1; b/=2){
            while (!ok(x+b)) x+=b;
        }
        int k = x+1;

        A busca encontra o maior valor de x para o qual ok(x) é falso. Portanto, o próximo valor k = x+1 é o menor valor possível para o qual ok(k) é verdadeiro. O tamanho inicial do salto, z, deve ser suficientemente grande, por exemplo, algum valor para o qual sabemos de antemão que ok(z) é verdadeiro.
        O algoritmo chama a função ok O(log z) vezes, portanto, a complexidade de tempo total depende da função ok. Por exemplo, se a função funciona em tempo O(n), a complexidade de tempo total é O(n log z).

    Encontrando o valor máximo:
        Busca binária também pode ser usada para encontrar o valor máximo de uma função que primeiro aumenta e depois diminui. Nossa tarefa é encontrar uma posição k tal que:
        -> f(x) < f(x+1) quando x < k, e
        -> f(x) > f(x+1) quando x >= k.

        A ideia é usar busca binária para encontrar o maior valor de x para o qual f(x) < f(x+1). Isso implica que k = x=1, porque f(x+1) > f(x+2). O seguinte código implementa a busca:
        
        int x = -1;
        for(int b = z; b >= 1; b /= 2){
            while(f(x+b) < f(x+b+1)) x+= b;
        }
        int k = x+1;

        Observe que, ao contrário da busca binária comum, aqui não é permitido que valores consecutivos da função sejam iguais. Nesse caso, não seria possível saber como continuar a busca.
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 4:

->Estruturas de dados:
    Uma estrutura de dado é uma maneira de se armazenar dados na memória do computador. É importante escolher uma estrutura de dado apropriada para o problema, porque cada estrutura possui suas próprias vantagens e desvantagens. A questão crucial é: quais operações são eficientes na estrutura de dado escolhida?
    Esse capítulo introduz as mais importantes estruturadas de dados que estão na biblioteca padrão do C++. É uma boa ideia usar a biblioteca padrão sempre que possível, porque isso salva tempo. Mais tarde nesse livro, aprenderemos mais sobre estruturas de dados mais sofisticadas que não estão disponíveis na biblioteca padrão.

    Array dinâmicos:
        Um array dinâmico é um array qual pode ter seu tamanho alterado durante a execução do programa. O array dinâmico mais popular em C++ é a estrutura vetor, qual pode ser usada praticamente como um array qualquer.
        O seguinte código cria um vetor vazio e adiciona 3 elementos nele:

        vector<int> vi;
        vi.push_back(3); //[3]
        vi.push_back(2); //[3,2]
        vi.push_back(5); //[3,2,5]

        Após isso, os elementos podem ser acessados como um array comum:

        cout << v[0] << "\n"; //3
        cout << v[1] << "\n"; //2
        cout << v[2] << "\n"; //5

        A função size returna a quantidade de elementos em um vetor. O seguinte código percorre o vetor e imprime todos os elementos nele:

        for(int i = 0; i < vi.size(); i++){
            cout << v[i] << "\n";
        }

        Uma maneira mais fácil de percorrer o array segue no código abaixo:

        for(auto x : vi) {
            cout << x << "\n";
        }

        A função back retorna o último elemento no vetor, e a função pop_back remove o último elemento.

        vector<int> vi;
        v.push_back(5);
        v.push_back(2);
        cout << v.back() << "\n"; //2
        v.pop_back();
        cout << v.back() << "\n"; //5

        O seguinte código cria um vetor com 5 elementos:

        vector<int> v = {2,4,2,5,1};

        Outra maneira de criar um vetor é dar o número de elemento e o valor inicial para cada elemento:

        //tamanho 10, valor inicial 0
        vector<int> vi(10);

        //tamanho 10, valor inicial 5
        vector<int> vi(10, 5);

        A implementação interna de um vetor utiliza um array comum. Se o tamanho de um vetor aumentar e o array se tornar pequeno demais, um novo array com mais memória alocada é criado e todos os elementos são movidos para o novo array. No entanto, isso não acontece com frequência e a complexidade de tempo média do push_back é O(1).
        A estrutura de string também é um array dinâmico que pode ser usado quase como um vetor. Além disso, há uma sintaxe especial para as strings que não está disponível para as outras estruturas de dados. Strings podem ser combinadas usando o símbolo +. A função substr(k,x) retorna a substring que começa em k e possui tamanho x, e a função find(t) encontra a posição da primeira ocorrência da substring t.
        O seguinte código apresenta algumas operações com strings:

        string a  = "hatti";
        string b = a+a;
        cout << b << "\n"; //hattihatti
        b[5] = 'v';
        cout << b << "\n"; //hattivatti
        string c = b.substr(3,4);
        cout << c << "\n"; //tiva

    Estruturas Set:
        "Set" é uma estrutura de dado que mantém coleções de elementos. As operações básicas de sets são inserção de elemento, busca de elemento e remoção de elemento.
        A biblioteca padrão do C++ contém 2 implementações da estrutura "set": A estrutura "set" baseada em uma árvore binária balanceada e suas operação funcionam em tempo O(log n). A estrutura "unordered_set" usa hashing, e suas operações funcionam em tempo O(1) em média.
        A escolha de qual implementação de "set" usar muitas vezes é uma questão de gosto. O benefício da estrutura "set" é que ela mantém a ordem dos elementos e fornece funções que não estão disponíveis na "unordered_set". Por outro lado, "unordered_set" pode ser mais eficiente.
        O seguinte código cria um "set" que contém inteiros, e mostra algumas operações. A função insert adiciona um elemento ao "set", a função count retorna o número de ocorrências de um elemento no "set" e a função erase remove um elemento do "set".

        set<int> s;
        s.insert(3);
        s.inset(2);
        s.insert(5);
        cout << s.count(3) << "\n"; //1
        cout << s.count(4) << "\n"; //0
        s.erase(3);
        s.insert(4);
        cout << s.count(3) << "\n"; //0
        cout << s.count(4) << "\n" //1

        Um set pode ser usado como um vetor, mas não é possível acessar os elementos usando a notação []. O seguinte código cria um "set", imprime o número de elementos nele e percorre todos os elementos.

        set<int> s = {2,5,6,8};
        cout << s.size() << "\n"; //4
        for(auto x : s){
            cout << x << "\n";
        }

        Uma importante propriedade dos "sets" é que todos os elementos deles são distintos. Assim, a função count sempre retorna 0 (o elemento não está no "set") ou 1 (o elemento está no "set"), e a função insert nunca adiciona um elemento caso ele já esteja no "set". O seguinte código ilustra isso:

        set<int> s;
        s.insert(5);
        s.insert(5);
        s.insert(5);
        cout << s.count(5) << "\n"; //1

        C++ também contém as estruturas "multiset" e "unordered_multiset" que funcionam de forma semelhante a "set" e "unordered_set", mas podem conter múltiplas instâncias de um mesmo elemento. Por exemplo, no seguinte código todas as 3 instâncias do número 5 são adicionadas ao "multiset":

        multiset<int> s;
        s.insert(5);
        s.insert(5);
        s.insert(5);
        cout << s.count(5) << "\n"; //3

        A função erase remove todas as instâncias do elemento de um "multiset":
        s.erase(5);
        cout << s.count(5) << "\n"; //0

        Muitas vezes, apenas uma instância pode ser removida, qual pode ser feito com o código abaixo:
        
        s.erase(s.find(5));
        cout << s.count(5) << "\n"; //2

    Estruturas Map:
        Uma estrutura "map" é um array generalizado que consiste em pares chave-valor. Enquanto as chaves em um array comum são sempre inteiros consecutivos 0,1,...,n-1, onde n é o tamanho do array, as chaves no "map" podem ser de qualquer tipo de dado e não precisam ter valores consecutivos.
        A biblioteca padrão do C++ contém 2 implementações para o "map" que correspondem as implementações do "set": a estrutura "map" baseada em uma árvore binária balanceada e que o acesso de elementos leva tempo O(log n), enquanto a estrutura "unordered_map" usa hashing e o acesso dos elemento leva tempo O(1) em média.
        O seguinte código cria um "map" one as chaves são strings e os valores são inteiros:

        map<string,int> m;
        m["monkey"] = 4;
        m["banana"] = 3;
        m["harpsichord"] = 9;
        cout << m["banana"] << "\n"; // 3

        Se o valor da chave for requerido e o "map" não o conte, a chave é automaticamente adicionada ao "map" com o valor padrão. Por exemplo, no seguinte código, a chave "aybabtu" com valor 0 é adicionada ao "map":

        map<string,int> m;
        cout << m[aybabtu] << "\n"; // 0

        A função count checa se existe uma chave no "map":

        if(m.count(aybabtu)){
            //chave existe
        }

        O seguinte código imprime todas as chaves e valores do "map":
        
        for(auto x : m){
            cout << x.first << " " << x.second << "\n";
        }

    Iteradores e intervalos:
        Muitas funções da biblioteca padrão do C++ operam com iteradores. Um iterador é uma variável que aponta para um elemento em uma estrutura de dado.
        Os iteradores frequentemente usados begin e end definem o intervalo que contém todos os elementos de uma estrutura de dados. O iterador begin aponta para o primeiro elemento na estrutura de dados, e o iterador end aponta para a posição após o último elemento. A situação é a seguinte: 

        { 3, 4, 6, 8, 12, 13, 14, 17 }
          |                          |
        s.begin()                   s.end()

        Note a assimetria nos iteradores: s.begin() aponta para um elemento na estrutura de dados, enquanto s.end() aponta para fora da estrutura de dados. Portanto, o intervalo definido pelos iteradores é semi-aberto.

        ->Trabalhando com intervalos:
            Iterados usados em funções da biblioteca padrão do C++ que recebem um intervalo de elementos em uma estrutura de dados. Normalmente, queremos processar todos os elementos em uma estrutura de dados, então os iterados begin e end são fornecidos para a função.
            Por exemplo, o seguinte código ordena um vetor usando a função sort, então inverte a ordem dos elementos usando a função reverse, e finalmente embaralha a ordem dos elementos usando a função random_shuffle.

            sort(v.begin(), v.end());
            reverse(v.begin(), v.end());
            random_shuffle(v.begin(), v.end());

            Essas funções também podem ser utilizadas em um array comum. Nesse caso, as funções recebem ponteiros para o array ao invés de iteradores:

            sort(a, a+n);
            reverse(a, a+n);
            random_shuffle(a, a+n);

        ->Iteradores de Set:
            Iteradores são frequentemente usados para acessar elemento de um "set". O seguinte código cria um iterador it que aponta para o menor elemento em um set:
            
            set<int>::iterator it = s.begin();

            Uma maneira menor de escrever o mesmo código é a seguinte:

            auto it = s.begin();

            O elemento para qual o iterador aponta pode ser acessado usando o símbolo *. Por exemplo, o seguinte código imprime o primeiro elemento de um "set":

            auto it = s.begin();
            cout << *it << "\n";

            Iteradores podem ser movidos usando os operado ++ (avançar) e -- (voltar), significando que o iterador se move para o próximo ou elemento anterior no "set". 
            O seguinte código imprime todos os elementos de um "set" em ordem crescente:

            for(auto it = s.begin(); it != s.end(); it++){
                cout << *it << "\n";
            }

            O seguinte código imprime o maior elemento em um "set":

            auto it = s.end(); it--;
            cout << *it << "\n";

            A função find(x) retorna um iterador que aponta para o elemento cujo valor é x. No entanto se "set" não possuir x, o iterador será igual ao end().

            auto it = s.find(x);
            if(it == s.end()){
                // x não foi encontrado
            }

            A função lower_bound(x) retorna um iterador que aponta para o menor elemento de um "set" cujo valor é pelo menos x, e a função upper_bound(x) retorna um iterador para o menor elemento no "set" cujo valor é maior que x. Em ambas as funções, se tal elemento não existir, o valor de retorno será end. Essas funções não existem na estrutura unordered_set qual não mantém a ordem dos elementos.
            Por exemplo, o seguinte código encontra o elemento mais próximo de x:

            auto it = s.lower_bound(x);
            if(it == s.begin()) {
                cout << *it << "\n";
            } else if(it == s.end()){
                it--;
                cout << *it << "\n";
            } else{
                int a = *it; it--;
                int b = *it;
                if(x-b < a-x) cout << b << "\n";
                else cout << a "\n";
            }

            O código assume que o "set" não está vazio, e percorre todos os possíveis casos usando o iterador it. Primeiro, o iterador aponta para o menor elemento cujo valor é pelo menos x. Se it for igual a begin, o elemento correspondente é o mais próximo de x. Se it for igual a end, o maior elemento no set é o mais próximo de x. Se nenhum dos casos anteriores for verdadeiro, o elemento mais próximo de x será o elemento que corresponde a it ou ao elemento anterior.

    Outras estruturas:
        ->Bitset:
            O "bitset" é um array cujo valor é 0 ou 1. Por exemplo, o seguinte código cria um bitset que contém 10 elementos:

            bitset<10> s;
            s[1] = 1;
            s[3] = 1;
            s[4] = 1;
            s[7] = 1;
            cout << s[4] << "\n"; //1
            cout << s[5] << "\n"; //0

            O benefício de usar bitsets é que precisa de menos memória que arrays comuns, porque cada elemento do bitset usa 1 bit de memória. Por exemplo, se n bits são armazenados em um array de inteiros, 32n bits de memória serão usados, mas um bitset correspondente só usa n bits de memória. Em adição, o valor de um bitset pode ser eficientemente manipulado usando operadores de bit, o que torna possível otimizar algoritmos usando conjuntos de bits.
            O seguinte código mostra outra maneira de criar o bitset acima:
            
            bitset<10> s(string("0010011010)); //da direita para a esquerda
            cout << s[4] << "\n"; //1
            cout << s[5] << "\n"; //0

            A função count retorna a quantidade de números 1 em um bitset:

            bitset<10> s(string("0010011010"));
            cout << s.count() << "\n"; //4

            O seguinte código mostra exemplos de uso de operadores de bits:
            
            bitset<10> a(string("0010110110"));
            bitset<10> b(string("1011011000"));
            cout << (a&b) << "\n"; // 0010010000
            cout << (a|b) << "\n"; // 101101110
            cout << (a^b) << "\n"; //1001101110

        ->Deque:
            Deque é um array dinâmico cujo tamanho pode ser eficientemente alterado em ambas extremidades do array. Assim como um vetor, uma "deque" fornece as funções push_back e pop_back, mas também inclui as funções push_front e pop_front quais não estão disponíveis para um vetor.
            Uma deque pode ser usada da seguinte forma:

            deque<int> d;
            d.push_back(5); // [5]
            d.push_back(2); // [5, 2]
            d.push_front(3); // [3, 5, 2]
            d.pop_back();   // [3, 5]
            d.pop_front(); //[5]

            A implementação interna de uma deque é mais complexa que a deu um vetor, e po essa razão, uma deque é mais lenta que um vetor. Mesmo assim, adicionar ou remover elementos leva tempo O(1) em média para ambos os casos.

        ->Pilha:
            Uma pilha é uma estrutura de dado que fornece 2 operações em tempo O(1): adicionar um elemento ao topo e remover um elemento do topo. Isso só é acessar o elemento do topo de uma pilha.
            O seguinte código mostra como uma pilha pode ser usada:

            stack<int> s;
            s.push(3);
            s.push(2);
            s.push(5);
            cout << s.top(); //5
            s.pop();
            cout << s.top(); //5
            s.pop();
            cout << s.top(); //2

        ->Fila:
            Uma fila também fornece 2 operações em tempo O(1): adicionar um elemento ao final da fila e remover o primeiro elemento de uma fila. Só é possível acessar o primeiro e último elemento de uma fila.
            O seguinte código mostra como uma fila pode ser usada:

            queue<int> q;
            q.push(3);
            q.push(2);
            q.push(5);
            cout << q.front(); // 3
            q.pop();
            cout << q.front(); //2

        ->Fila de prioridade:
            A fila de prioridade sempre mantém um conjunto de elementos. As operações oferecidas são inserção e, dependendo do tipo de fila, busca e remoção do elemento mínimo ou máximo. Inserção e remoção levam tempo O(log n)m e busca leva tempo O(1).
            Embora um "set" ordenado suporta eficientemente todas as operações de uma fila de prioridade, o benefício de se usar uma fila de prioridade é que possui menos fatores constantes. Uma fila de prioridade é normalmente implementada usando um estrutura heap que é muito mais simples que um árvore binária balanceada usada no "set" ordenado.
            Por padrão, os elemento em uma fila de prioridade em C++ são ordenados em ordem decrescent, e é possível encontrar e remover o maior elemento na fila. O seguinte código ilustra isso:

            priority_queue<int> q;
            q.push(3);
            q,push(5);
            q.push(7);
            q.push(2);
            cout << q.top() << "\n'; // 7
            q.pop();
            cout << q.top() << "\n"; // 5
            q.pop();
            q.push(6);
            cout << q.top() << "\n"; // 6
            q.pop();

            Se quisermos criar uma fila de prioridade que pode encontrar e remover o menor elemento, podemos fazer da seguinte forma:

            priority_queue<int, vector<int>, greater<int>> q;

        ->Estruturas de dados baseadas em políticas:
            O compilador g++ também suporta estruturas de dados que não fazem parte da biblioteca padrão do C++. Tais estruturas são chamadas de estruturas de dados policy-based (baseadas em política). Para usar essas estruturas, as seguintes linhas precisam ser adicionadas ao código:
            
            #include <ext/pb_ds/assoc_container.hpp>
            using namespace __gnu_pbds;

            Após isso, podemos definir a estrutura de dados indexed_set que é como um "set", mas pode ser indexada como um array. A definição para um valor inteiro é a seguinte:

            typdef tree<int, null_type, less<int>, rb_tree_tag, tree_order_statistics_node_update> indexed_set;

            Agora podemos criar um "set" da seguinte forma:

            indexed_set s;
            s.insert(2);
            s.insert(3);
            s.insert(7);
            s.insert(9);

            A particularidade desse "set" é que temos acesso aos índices que os elementos teriam em um array ordenado. A função find_by_order retorna um iterador para o elemento em uma determinada posição:

            auto x = s.find_by_order(2);
            cout << *x << "\n"; //7

            E a função order_of_key retorna a posição de um elemento dado:
            
            cout << s.order_of_key(7) << "\n"; // 2

            Se o elemento não aparece no "set", então nós recebemos a posição que ele teria no "set":

            cout << s.order_of_key(6) << "\n"; //2
            cout << s.order_of_key(8) << "\n"; //3

            Ambas as funções trabalham em tempo logarítmico.

    Comparação com ordenação:
        É frequentemente possível resolver problemas usando tanto estruturas de dados quanto ordenação. Às vezes, existem diferenças notáveis na eficiência real dessas abordagens, que podem estar ocultas em suas complexidades de tempo.
        Vamos considerar um problema onde são dadas 2 listas A e B, ambas contendo n elementos. Nossa tarefa é calcular o número de elementos que pertencem a ambas as listas. Por exemplo, as listas

        A = [5,2,8,9,4] e B = [3,2,9,5],

        a resposta seria 3, porque os número 2, 5 e 9 pertencem a ambas as listas.
        Uma solução direta para o problema é percorrer todos os pares de elementos em tempo O(n²), mas a seguir vamos focar em algoritmos mais eficientes.

        ->Algoritmo 1:
            Construímos um conjunto de elementos que aparecem em A, e após isso, percorremos os elementos de B e checamos se cada elemento também pertence a A. Isso é eficiente porque os elemento de A estão em um conjunto. Usando a estrutura "set", a complexidade de tempo do algoritmos é O(nlogn).
        
        ->Algoritmo 2:
            Não é necessário manter o conjunto ordenado, então ao invés de usar a estrutura "set" podemos usar, também, a estrutura "unordered_set". Essa é uma maneira fácil de tornar o algoritmo mais eficiente, porque só precisamos alterar a estrutura de dados subjacente. A complexidade de tempo do novo algoritmo é O(n).

        ->Algoritmo 3:
            Ao invés de usar estruturas de dados, podemos usar ordenação. Primeiro, ordenamos ambas as listas A e B. Após isso, percorremos ambas as listas ao mesmo tempo e verificamos os elementos em comum. O tempo de complexidade da ordenação é O(nlogn), e o resto do algoritmo trabalha em tempo O(n), então o tempo total é O(nlog).

        ->Comparação de eficiência:
            A seguinte tabela mostra quão eficiente os algoritmos acima são quando n varia e os elementos das listas são inteiros aleatórios entre 1 e 10^9:

              n|  Algoritmo 1 | Algoritmo 2 | Algoritmo 3
           10^6|          1.5s|         0.3s|        0.2s
         2*10^6|          3.7s|         0.8s|        0.3s
         3*10^6|          5.7s|         1.3s|        0.5s
         4*10^6|          7.7s|         1.7s|        0.7s
         5*10^6|         10.0s|         2.3s|        0.9s

            Os algoritmos 1 e 2 são iguais exceto que usam estruturas de "set" diferentes. Nesse problemas, essa escolha tem um importante efeito no tempo de execução, porque o algoritmo 2 é 4-5 vezes mais rápido que o primeiro.
            De qualquer forma, o mais eficiente foi o algoritmo, qual usa ordenação. Ele apenas usa metade do tempo quando comparado com o algoritmo 2. Curiosamente, a complexidade tanto para o algoritmo 1 quanto para o algoritmo 3 é O(n log n), mas apesar disso, o algoritmo 3 é dez vezes mais rápido. Isso pode ser explicado pelo fato de que a ordenação é um procedimento simples e é feito apenas uma vez no início do algoritmo 3, e o restante do algoritmo funciona em tempo linear. Por outro lado, o algoritmo 1 mantém uma árvore binária balanceada complexa durante todo o algoritmo.

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 5:

    ->Busca completa:
        Uma busca completa é um método geral que pode ser usado para resolver qualquer problema algorítmico. A ideia é gerar todas as possíveis soluções para o problema usando força bruta, então selecionar a melhor seleção ou contar o número de soluções, dependendo do problema.
        A busca completa é uma boa técnica se existir tempo suficiente para percorrer todas as solução, porque a busca é mais fácil de implementa e na maioria das vezes dá a resposta correta. Se a busca completa for lenta demais, outras técnica, como o algoritmos gulosos ou programação dinâmica podem ser necessários.

        ->Gerando subconjuntos:
            Primeiro, consideramos o problema de gerar todos os subconjunto de um conjunto com n elementos. Por exemplo, os subconjuntos de {0,1,2} são 0, {0}, {1}, {2}, {0,1}, {0,2}, {1,2}e {0,1,2}. Existem dois métodos comuns de gerar todos os subconjuntos: nós podemos realizar uma busca recursiva ou explorar a representação binária de inteiros.
            
            ->Método 1:
                Uma maneira elegante de percorrer todos os subconjuntos de um conjunto é usar recursividade. A seguinte função "search()" gera os subconjuntos do conjunto {0,1,...,n-1}. A função mantém um vetor "subset" que terá todos os elementos de cada subconjunto. A busca começa quando a função é chamada com o parâmetro 0.

                void search(int k){
                    if(k==n){
                        // processa o subconjunto
                    } else {
                        search(k+1);
                        subset.push_back(k);
                        search(k+1);
                        subset.pop_back();
                    }
                }

                Quando a função "search" é chamada com o parâmetro k, ela decide se deve include o elemento k no subconjunto ou não, e em ambos os casos, ela chama a si mesma como o parâmetro k+1. No entanto, se k=n, a função percebe que todos os elementos foram processados e um subconjunto foi gerado.
                A seguinte árvore ilustra a função chamada quando n = 3. Podemos sempre escolher ou o ramo esquerdo (k não está incluído no subconjunto) ou o ramo direito (k está incluído no subconjunto).

                (imagem pg 58 pdf)

            ->Método 2:
                Outra maneira de gerar subconjuntos é baseado na representação binária de inteiros. Cada subconjunto de um conjunto de n elementos pode ser representado como uma sequência de n bits, qual corresponde a um inteiro entre 0 ... 2^n-1. Os "uns" na sequência de bits indicam quais elementos estão no conjunto.
                A convenção usual é que o último bit corresponde ao elemento 0, o penúltimo bit corresponde ao elemento 1 e assim por diante. Por exemplo, a representação em bits de 25 é 11001, qual corresponde ao subconjunto {0,3,4}.
                O seguinte código percorre todos os subconjuntos de um conjunto de n elementos:

                for(int b = 0; b < (1<<n); b++){
                    //processa o subconjunto
                }

                O código a seguir mostra como encontramos elementos de um subconjunto que corresponda a uma sequência de bits. Quando processamos cada subconjunto, o código constrói um vetor que contém os elemento no subconjunto.

                for(int b = 0; b < (1<<b); b++){
                    vector<int> subset;
                    for(int i = 0; i < n; i++){
                        if(b&(1<<i)) subset.push_back(i);
                    }
                }

        ->Gerando permutações:
            A seguir, consideramos o problema e gerar todas as permutações de um conjunto de n elementos. Por exemplo, as permutações de {0,1,2} é (0,1,2), (0,2,1), (1,0,2), (1,2,0), (2,0,1) e (2,1,0). Novamente, existem dois tipos de abordagens: podemos ou usar recursividade ou percorrer todas as permutações de forma interativa.

            ->Método 1:
                Como subconjuntos, permutações podem ser geradas usando recursividade. A seguinte função "search" percorre todas as permutações de um conjunto {0,1,...,n-1}. A função constrói um vetor "permutation" que contém a permutação, e a busca começa quando a função é chamada sem parâmetros.
                
                void search(){
                    if(permutation.size() == n){
                        // processa a permutação
                    } else{
                        for(int i = 0; i < n; i++){
                            if(chosen[i]) continue;
                            chosen[i] = true;
                            permutation.push_back(i);
                            search();
                            chosen[i] = false;
                            permutation.pop_back();
                        }
                    }
                }

                Cada chamada de função adiciona um novo elemento ao vetor "permutation". O array "chosen" indica quais elementos já foram incluídos na permutação. Se o tamanho de "permutation" for igual ao tamanho do conjunto, a permutação foi gerada.

            ->Método 2:
                Outro método de gerar permutações é começar com a permutação (0,1,...,n-1) e repetidamente usar a função que constrói a próxima permutação na ordem crescente. A biblioteca padrão do C++ contém a função "next_permutation" que pode ser usada para isso:
                
                vector<int> permutation;
                for(int i = 0; i < n; i++){
                    permutation.push_back(i);
                }
                do{
                    //processa a permutação
                } while(next_permutation(permutation.begin(), permutation.end()));

        ->Backtracking:
            Um algoritmo de backtracking começa com uma solução vazia e a estende passo a passo. A busca recursiva percorre todas os diferentes maneiras de como a solução pode ser construída.
            Como exemplo, considere o problema de calcular o número de maneiras em que n rainhas possam ser colocadas em um tabuleiro de xadrez n x n de forma que nenhuma das rainhas não ataque as outras. Por exemplo, quando n = 4, existem duas possíveis soluções:

            (imagens pg 60 pdf).

            O problema pode ser resolvido usando a técnica de backtracking, colocando as rainhas no tabuleiro linha por linha. Mais precisamente, exatamente uma rainha será colocada em cada linha, de modo que nenhuma rainha ataque qualquer uma das rainhas colocadas anteriormente. Uma solução é encontrada quando todas as n rainhas foram colocadas no tabuleiro.
            Por exemplo, quando n = 4, algumas soluções parciais geradas pelo algoritmo backtracking seriam da seguinte forma:

            (imagens pg 60 pdf).

            No nível mais abaixo, as 3 primeiras configurações são ilegais, porque as rainhas se atacam. No entanto, a quarta configuração é válida e pode se estender para uma solução completa colocado mais 2 rainhas no tabuleiro. Existe apenas uma maneira de colocar as 2 rainhas restantes.
            O algoritmo pode ser implementado da seguinte forma:

            void search(int y){
                if(y == n){
                    count++
                    return;
                }
                for(int x = 0; x < n; x++){
                    if(column[x] || diag1[x+y] || diag2[x-y+n-1]) continue;
                    column[x] = diag1[x+y] = diag2[x-y+n-1] = 1;
                    search(y+1);
                    column[x] = diag1[x+y] = diag2[x-y+n-1] = 0;
                }
            }

            A busca começa chamando "search(0)". O tamanho do tabuleiro é n × n, e o código calcula o número de soluções em "count".
            O código assume que as linhas e colunas do tabuleiro foram numerada de 0 até n-1. Quando a função "search" é chamada com parâmetro y, colocando uma rainha na linha y e chamando ela mesma com o parâmetro y+1. Então, se y = n, a solução foi encontrada e a variável "count" é incrementada em um.
            O array "column" mantém o controle das colunas que contém uma rainha, e os arrays diag1 e diag2 mantêm o controle das diagonais. Não é permitido adicionar outra rainha a uma coluna ou diagonal que já contenha uma rainha. Por exemplo, as colunas e diagonais de um tabuleiro 4x4 pode ser numerada da seguinte forma:

            (imagem pg 61 pdf)

            Deixe q(n) denotar o número de maneiras de colocar n rainhas em um tabuleiro de xadrez n x n. O algoritmo de backtracking acima nos diz, por exemplo, q(8) = 92. Quando n aumenta, a busca rapidamente se torna lenta, porque o número de soluções cresce exponencialmente. Por exemplo, calculando q(16) = 14772512 usando o algoritmo acima leva cerca de um minuto em um computador moderno. 

        ->Podando a busca:
            Podemos frequentemente otimizar o backtracking podando a árvore de busca. A ideia é adicionar "inteligência" ao algoritmo, para que ele note o mais cedo possível que a solução parcial não pode ser estendida em uma solução completa. Essas otimizações podem ter um efeito significativo na eficiência da busca.
            Consideremos o problema de calcular o número de caminhos em uma grade n x n, do canto superior esquerdo ao canto inferior direito, de forma que o caminho visite cada quadrado exatamente uma vez. Por exemplo, um gride 7x7, teria 111712 possíveis caminhos. Um dos caminhos é o seguinte:

            (imagem pg 62 pdf)

            Focaremos no caso 7x7, porque é um nível de dificuldade apropriado para nossas necessidades. Começamos com uma solução direta com um algoritmo backtracking, e então otimizaremos passo a passo utilizando observações de como a busca pode ser podada. Após cada otimização, mediremos o tempo de execução do algoritmo e o número de chamadas recursivas, dessa maneira podemos ver claramente os efeitos de otimização e a eficiência da busca.

            ->Algoritmo básico:
                A primeira versão do algoritmos não contém otimizações. Nós simplesmente utilizaremos backtracking para gerar todos os possíveis caminhos do canto superior esquerdo até o canto inferior direito e contar o número de caminhos possíveis.
                • Tempo de execução: 483 segundos
                • Número de chamadas recursivas: 76 bilhões

            ->Otimização 1:
                Em qualquer solução, primeiro movemos um passo para baixo ou direita. Sempre existem dois caminhos que são simétricos em relação à diagonal da grade após o primeiro passo. Por exemplo, os seguintes caminhos são simétricos:

                (imagens pag 62)

                Portanto, podemos decidir que sempre damos o primeiro passo para baixo (ou para direita) e, finalmente, multiplicamos o número de soluções por dois.

                • Tempo de execução: 244 segundos
                • Número de chamadas recursivas: 38 bilhões

            ->Otimização 2:
                Se o caminho chegar ao quadrado inferior direito antes de ter visitado todos os outros quadrados do gride, está claro que não é possível obter uma solução completa. Um exemplo disso é o seguinte caminho:

                (imagem pag 63)

                Usando essa observação, podemos finalizar a busca imediatamente se chegarmos no quadrado inferior direito muito cedo.
                
                • Tempo de execução: 119 segundos
                • Número de chamadas recursivas: 20 bilhões

            ->Otimização 3:
                Se o caminho tocar uma parede e puder virar para a direita ou esquerda, a grade e divide em duas partes que contém quadrados não visitados. Por exemplo, a seguinte situação, o caminho pode virar ou para esquerda ou para a direita:

                (imagem pag 63 pdf)

                Nesse caso, nós não podemos mais visitar todos os quadrados, então podemos finalizar a busca. A otimização é muito útil:

                • Tempo de execução: 1.8 segundos
                • Número de chamadas recursivas: 221 milhões

            ->Otimização 4:
                A ideia da otimização 3 pode ser generalizada: se o caminho não pode continuar para a frente, mas pode virar para a esquerda ou para a direita, a grade se divide em duas partes que contêm quadrados não visitados. Por exemplo, considere o seguinte caminho:

                (imagem pag 64 pdf)

                Está claro que não podemos mais visitar todos os quadrado, então podemos finalizar a busca. Após essa otimização, a busca se torna muito eficiente:
                
                • Tempo de execução: 0.6 segundos
                • Número de chamadas recursivas: 69 milhões

                Agora é um bom momento para parar de otimizar o algoritmo e ver o que conquistamos. O tempo de execução originar do algoritmo era de 483 segundos, e agora, após as otimizações, o tempo de execução é de apenas 0.6 segundos. Assim, o algoritmo se tornou cerca de 1000 vezes mais rápido após as otimizações.
                Isso é um fenômeno comum em backtracking, porque a árvore de busca geralmente é grande e até mesmo observações simples podem efetivamente podar a busca. Otimizações que ocorrem durante os primeiros passos do algoritmo, ou seja, no topo da árvore de busca, são especialmente úteis.

            ->Meet in the middle(encontro no meio):
                Meet in the middle(encontro no meio) é uma técnica onde o espaço de busca é dividido em duas partes de tamanho aproximadamente igual. Uma busca separada é performada em ambas as partes, e finalmente os resultados das buscas são combinados.
                A técnica pode ser usada se houver uma maneira eficiente de combinar o resultado das buscas. Em tal situação, as duas buscas devem exigir menos tempo que uma grande busca. Normalmente, podemos tornar o fator 2^n em um fator 2^n/2 usando a técnica "meet in the middle".
                Como exemplo, considere um problema em que temos uma lista de n números e um número x, e queremos descobrir se é possível escolher alguns números da lista de forma que a soma deles seja x. Por exemplo, dada a lista [2,4,5,9] e x = 15, podemos escolher os números [2,4,9] para obter 2+4+9 = 14. No entanto, se x = 10 na mesma lista, não é possível formar a soma.
                Um simples algoritmo para o problema é percorrer todos os subconjuntos dos elementos e verificar se a soma em algum dos subconjuntos é x. O tempo de execução de tal algoritmo é O(2^n), por que existirão 2^n subconjuntos. No entanto, usando a técnica "meet in the middle", podemos alcançar um tempo mais eficiente, esse sendo O(2^n/2). Note que O(2^n) e O(2^n/2) são complexidade diferente porque 2^n/2 é igual a √(2^n).
                A ideia é dividir a lista em duas listas A e B tal que duas listas contenham cerca de metade dos números. A primeira busca gera todos os subconjuntos de A e armazena suas somas em uma lista SA. De maneira correspondente, a segunda busca cria uma lista SB a partir de B. Após isso, é suficiente verificar se é possível escolher um elemento de SA e outro elemento de SB de forma que a soma deles seja x. Isso é possível exatamente quando há uma maneira de formar a soma x usando os números da lista original.
                Por exemplo, suponha que a lista seja [2,4,5,9] e x = 15. Primeiro, dividimos a lista em A = [2,4] e B = [5,9]. Após isso, criamos SA = [0,2,4,6] e SB = [0,5,9,14]. Nesse caso, a soma x = 15 é possível de ser formada, porque SA contém o número 6, SB contém o número 9, e 6+9=15. Isso corresponde a solução [2,4,9].
                Podemos implementar o algoritmo de forma que sua complexidade de tempo seja O(2^n/2). Primeiro, geramos listas ordenadas SA e SB, o que pode ser feito em tempo O(2^n/2) usando uma técnica semelhante a merge. Depois disso, como as listas estão ordenadas, podemos verificar em tempo O(2^n/2) se a soma x pode ser criada a partir de SA e SB.

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 6:

    ->Algoritmos gulosos:
        Um algoritmo guloso constrói a solução para o problema sempre fazendo a escola que parece a melhor para o momento. Um algoritmo guloso nunca desfaz suas escolhas, mas constrói diretamente a solução final. Por essa razão, algoritmos gulosos são muito eficientes normalmente.
        A dificuldade em projetar um algoritmo guloso é encontrar a estratégia que sempre produz uma ótima solução para o problema.  As escolhas localmente ótimas em um algoritmo guloso também devem ser globalmente ótimas. É frequentemente difícil argumentar que um algoritmo guloso funciona.

        ->Problema da moeda:
            Como primeiro exemplo, consideremos um problema onde dado um conjunto de moedas, nossa tarefa é formar a soma de dinheiro "n" usando as moedas. Os valores das moedas são coins={c1,c2,...,ck}, e cada moeda pode ser usada quantas vezes quisermos. Qual o menor número de moedas necessário?
            Por exemplo, se as moedas são euros (em centimos)

            {1,2,5,10,20,50,100,200}

            e n = 520, nós precisaremos de pelo menos 4 moedas. A solução ótima é selecionar as moedas 200 + 200 + 100 + 20 cuja soma é 520.

            ->Algoritmo guloso:
                Um simples algoritmo guloso para o problema sempre seleciona a maior moeda possível, até que a soma requisitada tenha sido construída. Esse algoritmo funciona no caso de exemplo, porque primeiro selecionamos 2 moedas de 200 centimos, então uma de 100 centimos e, finalmente uma de 20 centimos. Mas esse algoritmo sempre funciona?
                Acontece que se as moedas forem as moedas de euro, o algoritmo guloso sempre funciona, ou seja, ele sempre produz uma solução ótima com o menor número possível de moedas. 
                A correção do algoritmo pode ser mostrada da seguinte forma:
                Primeiro, cada moeda 1, 5, 10, 50, e 100 aparece no máximo uma vez em uma solução ótima, porque se a solução contivesse duas moedas desse tipo, nós poderíamos repor por uma única moeda e obter uma solução melhor. Por exemplo, se a solução contém moedas 5+5, podemos substituí-la por uma moeda de 10.
                No mesmo sentido, moedas de 2 e 20 aparecem no máximo duas vezes em uma solução ótima, porque podemos substituir as moedas 2 + 2 + 2 pelas moedas 5 + 1 e as moedas 20 + 20 + 20 pelas moedas 50 + 10. Além disso, uma solução ótima não pode conter moedas 2 + 2 + 1 ou 20 + 20 + 10, porque poderíamos substituí-las por moedas de 5 e 50.
                Usando essas observações, mostra-se que para cada moeda x que não é possível construir de maneira otimizada uma soma x ou qualquer soma maior usando apenas moedas menores que x. Por exemplo, se x = 100, a maior soma otimizada usando as moedas menores é 50 + 20 + 20 + 5 + 2 + 2 = 99. Assim, o algoritmo guloso sempre seleciona a maior moeda para produzir uma solução ótima.
                Esse exemplo mostra que pode ser difícil argumentar que um algoritmo guloso funciona, mesmo se o algoritmo em si for simples.

            ->Caso geral:
                No caso geral, o conjunto de moedas pode conter quaisquer moedas e o algoritmo guloso não necessariamente produz uma solução ótima.
                Podemos provar que um algoritmo guloso não funciona mostrando um contraexemplo onde o algoritmo fornece uma resposta errada. Nesse problema, podemos facilmente encontrar um contraexemplo: se as moeda forem {1,3,4}, e a soma é 6, o algoritmo guloso produz a solução 4+1+1 enquanto a solução ótima é 3+3.
                Não se sabe se o problema geral da moeda pode ser resolvido usando qualquer algoritmo guloso. No entanto, como veremos no capítulo 7, em alguns casos, o problema geral pode ser eficientemente resolvido usando um algoritmo de programação dinâmica que sempre nos dá a resposta correta.
        
        ->Agendamento:
            Muitos problemas de agendamento podem ser resolvidos usando algoritmos gulosos. Um clássico é o problema seguinte: dado n eventos com seus horários de início e fim, encontre a agenda que inclui o máximo de eventos possíveis. Não é possível selecionar parcialmente um evento. Por exemplo, considere os seguintes eventos:

            evento  |  horário de início  |  horário de fim
              A     |          1          |        3
              B     |          2          |        5
              C     |          3          |        9
              D     |          6          |        8

            Nesse caso, o máximo de evento é 2. Por exemplo, podemos selecionar os eventos B e D como segue:

            (imagem 1 pg 69 pdf)

            É possível criar vários algoritmo gulosos para o problema, mas qual deles funciona em todos os casos?

            ->Algoritmo 1:
                A primeira ideia é selecionar eventos o mais curto possíveis. No caso exemplo, esse algoritmo seleciona os seguintes eventos:

                (imagem 2 pg 69 pdf)

                No entanto, selecionar os eventos mais curtos não é sempre a estratégia correta. Por exemplo, se o algoritmo cair no seguinte caso:

                (imagem 3 pg 69 pdf)

                Se selecionarmos o evento mais curto, apenas selecionaríamos um evento. No entanto, seria possível selecionar ambos os eventos longos.

            ->Algoritmo 2:
                Outra ideia é sempre selecionar o próximo possível evento que começa o mais cedo possível. Esse algoritmo seleciona os seguintes eventos:

                (imagem 4 pg 69 pdf)

                No entanto, podemos encontrar, também, um contraexemplo para esse algoritmo. Por exemplo, no seguinte caso, o algoritmo selecionaria apenas um evento:

                (imagem 5 pg 69 pdf)

                Se selecionarmos o primeiro evento, não é possível selecionar nenhum dos outros eventos. No entanto, seria possível selecionar os outros 2 eventos.

            ->Algoritmo 3:
                A terceira ideia é sempre selecionar o próximo evento possível que termina o mais cedo possível. O algoritmo seleciona os seguintes eventos:

                (imagem 1 pg 70 pdf)

                Descobriu-se que esse algoritmo sempre produz uma solução ótima. A razão para isso é que é sempre uma escolha ótima selecionar primeiro um evento que termine o mais cedo possível. Após isso, é uma ótima escolha selecionar o próximo evento usando a mesma estratégia, etc., até não pudermos selecionar mais evento algum.
                Uma maneira de argumentar que o algoritmo funciona é considerar o que acontece se primeiro selecionarmos um evento que termina mais tarde do que o evento que termina o mais cedo possível. Agora, teremos no máximo um número igual de opções sobre como podemos selecionar o próximo evento. Portanto, selecionar um evento que termina mais tarde nunca pode levar a uma solução melhor, e o algoritmo guloso é correto.
        
        ->Tarefas e prazos:
            Consideremos um problema onde dada n tarefas com duração e prazos, nossa tarefa é escolher um ordem para performar as tarefas. Para cada tarefa, conseguimos d-x pontos onde d é o prazo da tarefa e x o momento em que finalizamos a tarefa. Qual a pontuação máxima que é possível obter?
            Por exemplo, suponha que as tarefas sejam as seguintes:

            tarefa | duração | prazo
               A   |    4    |  2
               B   |    3    |  5
               C   |    2    |  7
               D   |    4    |  5

            Nesse caso, um cronograma ótimo para as tarefas é o seguinte:

            (imagem 3 pg 70 pdf)

            Nessa solução, C rende 5 pontos, B rende 0 pontos, A rende -7 pontos e D rende -8 pontos, então resultando uma pontuação de -10.
            Surpreendentemente, a solução ótima para o problema não depende dos prazos, mas uma estratégia gananciosa correta é simplesmente realizar as tarefas ordenadas por suas durações em ordem crescente. A razão para isso é que se realizarmos 2 tarefas consecutivamente tal que a primeira leva mais tempo que a segunda, podemos obter uma melhor solução se trocarmos as tarefas de posição. Por exemplo, considere o seguinte cronograma:

            (imagem 1 pg 71 pdf)

            Aqui a > b, então se trocarmos as tarefas:

            (imagem 2 pg 71 pdf)

            Agora, X fornece b ponto A menos e Y fornece a pontos a mais, então a pontuação total aumenta por a - b > 0. Em uma solução ótima, para quaisquer 2 tarefas consecutivas, é realizar a tarefa menor antes da tarefa maior. Assim, as tarefas podem ser ordenadas pelas suas durações.

        ->Minimizando somas:
            Consideremos agora o problema em que dados n números, a1, a2, ..., an, nossa tarefa é encontrar um valor x que minimize a soma
            | a1 - x | ^ C + | a2 - x | ^ C + ... + | an - x | ^ C

            Vamos focar nos casos em que c = 1 e c = 2.

            ->Caso c = 1
                Nesse caso, nós devemos minimizar a soma 
                | a1 - x | + | a2 - x | + ... + | an - x |

                Por exemplo, se os números forem [1,2,9,2,6], a melhor solução é selecionar x = 2, que produz a soma
                |1-2| + |2-2| + |9-2| + |2-2| + |6-2| = 12

                No caso geral, a melhor escolha é que x seja a mediana dos números, ou seja, o número do meio após a ordenação. Por exemplo, a lista [1,2,9,2,6] se torna [1,2,2,6,9] após a ordenação, então a mediana é 2.
                A mediana é a escolha ótima, porque se x é menor que a mediana, a soma se torna menor aumentando x, e se x é menor que a medina, a soma se torna menor diminuindo x. Portanto, a solução ótima é x ser a mediana. Se n for par e existirem duas medianas, ambas as medianas e todos os valores entre elas são ótimas escolhas.

            ->Caso c = 2
                Nesse caso, devemos minimizar a soma
                (a1-x)² + (a2-x)² + ... + (an - x)²

                Por exemplo, se os números forem [1,2,9,2,6], a melhor solução seria selecionar 4, qual produz a soma

                (1-4)² + (2-4)² + (9-4)² + (2-4)² + (6-4)² = 46

                No caso geral, a melhor escolha para x éa média dos números. No exemplo, a média é (1+2+9+2+6)/5 = 4. Esse resultado pode ser obtido apresentando a soma da seguinte forma:

                (fórmula 1 pg 72 pdf)

                A última parte não depende de x, então podemos ignorá-la. As partes restantes formam uma função nx² -2xs onde s = a1+a2+...+an. Isso é uma parábola que se abre para cima, com raízes x = 0 e x = 2s/n, e o valor mínimo é a média das raízes x = sn, ou seja a média dos números a1, a2, ..., an.

        ->Compressão de dados:
            Um código binário atribui a cada caractere de uma sequência uma palavra-código que consiste em bits. Podemos comprimir a string usando o código binário substituindo cada caractere para a palavra-código correspondente. Por exemplo, o seguinte código binário atribui palavras-código para os caracteres A-D:

            caractere | palavra-código
                    A |             00
                    B |             01
                    C |             10
                    D |             11

            Este é um código de tamanho constante, o que significa que o tamanho de cada palavra-código é o mesmo. Por exemplo, podemos comprimir a sequência AABACDACA da seguinte forma:

            000001001011001000

            Usando esse código, o tamanho da string comprimida é de 18 bits. No entanto, podemos comprimir melhor a string se usarmos um código de tamanho variável, onde as palavras-código podem ter diferentes tamanhos. Assim, podemos dar palavras de código curtas para caracteres que aparecem frequentemente e palavra-código longas para caracteres que aparecem raramente. Descobre-se que um código ótimo para a string acima é o seguinte:

            caractere | palavra-código
                    A |              0
                    B |            110
                    C |             10
                    D |            111

            Um código ótimo produz uma string comprimida que é a mais curta possível. Neste caso, a string comprimida usando o código ótimo é:

            001100101110100,
            
            então apenas 15 bits são necessários ao invés de 18. Assim, graças ao código melhorado, é possível salva 3 bits em uma string comprimida.
            Nós exigimos que nenhuma palavra-código seja um prefixo de outra palavra-código. Por exemplo, não é permitido que um código contenha as palavras-código 10 e 1011. A razão para isso é que queremos ser capazes de gerar a string original a partir da string comprimida. Se uma palavra-código pudesse ser um prefixo de outra palavra-código, isso nem sempre seria possível. Por exemplo, o seguinte código não é válido:

            caractere | palavra-código
                    A |             10
                    B |             11
                    C |           1011
                    D |            111
                
            Usando esse código, não seria possível saber se a string comprimida 1011 corresponde a AB ou C.

            ->Código de Huffman:
                O código de Huffman é um algoritmo guloso que constrói um ótimo código para comprimir uma dada string. O algoritmo constrói uma árvore binária baseada na frequência dos caracteres na string, e cada palavra-código do caractere pode ser lida seguindo o caminho da raiz até o nó correspondente. Um movimento para a esquerda corresponde ao bit 0, e um movimento para a direita corresponde ao bit 1.
                inicialmente, cada caractere da string é representado por um nó cujo peso é igual a quantidade de vezes que o caractere aparece na string. Em seguida, em cada etapa, 2 nós com os menores pesos são combinados criando um novo só cujo peso é a soma dos pesos dos nós originais. O processo continua até que todos os nós tenham sido combinados.
                Em seguida, veremos como o código de Huffman cria um ótimo código para a string AABACDACA. Inicialmente, existem 4 nós que correspondem aos caracteres da string:

                (imagem 1 pag 73 pdf)

                O nó que representa o caractere A possui peso 5 poque o caractere A aparece 5 vezes na string. Os outros pesos foram calculados da mesma maneira.
                O primeiro passo é combinar os nós que correspondem aos caracteres B e D, ambos com peso 1. O resultado é:

                (imagem 2 pag 73 pdf)

                Após isso, os nós com peso 2 são combinados:

                (imagem 1 pag 74 pdf)

                Finalmente, os 2 nós restantes são combinados:

                (imagem 2 pag 74 pdf)

                Agora, todos os nós estão na árvore, então o código está pronto. as seguintes palavras chaves podem ser lidas da árvore:

                caractere | palavra-código
                    A |                 0
                    B |               110
                    C |                10
                    D |               111

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 7:
    
    ->Programação dinâmica:
        Programação dinâmica é uma técnica que combina a corretude da busca completa e eficiência dos algoritmos gulosos. Programação dinâmica pode ser aplicada se o problema puder ser dividido em subproblemas sobrepostos que podem ser resolvidos independentemente.
        Existes duas maneiras de se usar programação dinâmica:

        • Encontrar uma solução ótima: Queremos encontrar uma solução que seja tão grande quanto possível ou tão pequena quanto possível.

        • Contar o número de soluções: Queremos calcular o total de número de possíveis soluções.

        Primeiro veremos como a programação dinâmica pode ser usada para encontrar uma ótima solução, e então usaremos a mesma ideia para a contagem de soluções.
        Entenda que a programação dinâmica é um pilar para qualquer carreira na programação competitiva. Enquanto a ideia básica é simples, o desafio é como aplicar a programação dinâmica em diferentes problemas. Esse capítulo introduz um conjunto de problemas clássicos que são um bom ponto de partida.

        ->Problema da moeda:
            Primeiro focaremos em um problema que já vimos no capítulo 6: Dados um conjunto de valores de moedas coins = {c1,c2,...,ck} e o objetivo é a soma de dinheiro n, nossa tarefa é formar a soma n com o menor número de moedas possível.
            No capítulo 6, resolvemos o problema usando um algoritmos guloso que sempre escolhia a maior moeda possível. O algoritmo guloso funciona, por exemplo, quando as moedas são moedas de euro, mas, no caso geral, o algoritmo guloso nem sempre produz uma ótima solução.
            Agora é o momento de resolver o problema de forma eficiente usando programação dinâmica, então o algoritmo funcionará para qualquer conjunto de moedas. O algoritmo de programação dinâmica é baseado em uma função recursiva que percorre todas as possibilidade de como formar a soma, assim como um algoritmo de força bruta. No entanto, o algoritmo de programação dinâmica é eficiente porque usa memorização e calcula a resposta para cada subproblema apenas uma vez. 

            ->Formulação recursiva:
                A ideia da programação dinâmica é formular o problema recursivamente, de modo que a solução para o problema possa ser calculada a partir das soluções dos subproblemas menores. No problema da moeda, um problema recursivo natural é o seguinte: qual o menor número de moedas necessário para formar a soma x?
                Vamos denotar por solve(x) o número mínimo de número de moedas necessário para a soma x. Os valores da função dependem do valores das moedas. Por exemplo, se as moedas forem coins = {1,3,4}, os primeiros valores da função seriam os seguintes:

                solve(0) = 0
                solve(1) = 1
                solve(2) = 2
                solve(3) = 1
                solve(4) = 1
                solve(5) = 2
                solve(6) = 2
                solve(7) = 2
                solve(8) = 2
                solve(9) = 3
                solve(10) = 3

                Por exemplo, solve(10) = 3, porque pelo menos 3 moedas são necessárias para formar a soma 10. A solução ótima é 3 + 3 + 4 = 10.
                A propriedade essencial de solve é que seus valores podem ser recursivamente calculados de valores menores. A ideia é focar na primeira moeda que escolhemos para a soma. Por exemplo, no cenário acima, a primeira moeda pode ser ou 1 ou 3 ou 4. Se escolhermos primeiro a moeda 1, o resto da tarefa é formar a soma 9 usando o mínimo de moedas possível, qual é um subproblema do problema original. É claro que o mesmo se aplica às moedas 3 e 4. Assim, podemos usar a seguinte fórmula recursiva para calcular o número mínimo de moedas: 

                solve(x) = min(solve(x-1) + 1, solve(x-3) + 1, solve(x-4) + 1).

                O caso base para a recursividade é solve(0) = 0, porque não são necessárias moedas para formar uma soma vazia. Por exemplo,
                
                solve(10) = solve(7) + 1 = solve(4) + 2 = solve(0) + 3 = 3.

                Agora estamos preparados para oferecer uma função recursiva geral ue calcula o mínimo de moedas necessárias para formar uma soma x:

                (fórmula pag 76 pdf)

                Primeiro, se x < 0, o valor é INF, porque não é possível formar uma soma negativa de dinheiro. Em seguida, se x = 0, o valor é 0, porque não são necessárias moedas para formar uma soma vazia. Finalmente, se x > 0, a variável c percorre todas as possibilidades de como escolher a primeiro moeda da soma.
                Uma vez que a função recursiva que resolve o problema é encontrada, podemos diretamente implementar a solução em C++ (a contante INF denota o infinito):

                int solve(int x){
                    if(x < 0) return INF;
                    if(x = 0) return 0;
                    
                    int best = INF;

                    for(auto c : coins){
                        best = min(best, solve(x-c)+1);
                    }

                    return best;
                }

                Ainda assim, essa função não é eficiente, porque existirão números exponenciais de caminhos para construir a soma. No entanto, em seguida veremos como fazer essa função eficiente usando a técnica chamada memorização.

            ->Usando memorização:
                A ideia da programação dinâmica é usar memorização para calcular eficientemente o valor de uma função recursiva. Isso significa que o valor da função é armazenado em um array após ser calculado. Para cada parâmetro, o valor da função é calculado recursivamente apenas uma vez, e após isso, o valor pode ser diretamente tirado do array.
                Nesse problema, usaremos os arrays

                bool ready[N];
                int value[N];

                onde ready[x] indica se o valor de solve(x) foi calculado, e se foi, value[x] contém esse valor. A constante N foi escolhida de modo que todos os valores necessários caibam nos arrays.
                Agora a função pode ser eficientemente implementada da seguinte forma:

                int solve(int x){
                    if(x < 0) return INF;
                    if(x = 0) return 0;
                    if(ready[x]) return value[x];

                    int best = INF;

                    for(auto c : coins){
                        best = min(best, solve(x-c)+1);
                    }

                    value[x] = best;
                    ready[x] = true;
                    return best;
                }

                A função mantém os casos base x < 0 e x = 0 como antes. Em seguida, a função verifica de ready[x] se solve(x) já foi armazenado em value[x], e se foi, a função retorna diretamente o valor. Caso contrário, a função calcula o valor de solve(x) recursivamente e armazena ele em value[x].
                Essa função funciona eficientemente, porque a resposta para cada parâmetro x é calculado recursivamente apenas uma vez. Após o valor de solve(x) ser armazenado em value[x], pode ser eficiente recuperado sempre que a função for chamada novamente com o parâmetro x. A complexidade de tempo desse algoritmo é O(nk), onde n é a soma e k o número de moeda.
                Note que também podemos construir o array value de forma iterativa usando um loop que simplesmente calcula todos os valores de solve para os parâmetros 0...n:

                value[0] = 0;
                for(int x = 1 x <= n; x++){
                    value[x] = INF;
                    for(auto c : coins){
                        if(x-c >= 0){
                            value[x] = min(value[x], value[x-c]+1);
                        }
                    }
                }

                De fato, a maioria dos programadores competitivos prefere essa implementação, por que é menor e possui menor fatores constantes. De agora em diante, também usaremos implementações iterativas em nossos exemplos. Ainda assim, é frequentemente mais fácil pensar em soluções de programação dinâmica em termos de funções recursivas.
            
            ->Construindo uma solução:
                Às vezes, nos é solicitado tanto encontrar o valor de uma solução ótima quanto fornecer um exemplo de como essa solução pode ser construída. No problema das moedas, por exemplo, podemos declarar outro array que indica, para cada quantia de dinheiro, a primeira moeda em uma solução ótima:

                int first[N];

                Em seguida, modificamos o algoritmo da seguinte forma:

                value[0] = 0;
                for(int x = 1; x <= n; x++){
                    value[x] = INF;
                    for(auto c : coins){
                        if(x-c >= 0 && value[x-c]+1 < value[x]){
                            value[x] = value[x-c]+1;
                            first[x] = c;
                        }
                    }
                }

                Após isso, o seguinte código pode ser usado para imprimir as moedas que aparecem em uma solução ótima para a soma n:

                while(n > 0){
                    cout << first[n] << "\n";
                    n -= first[n];
                }

            ->Contando o número de soluções:
                Consideremos agora outra versão do problema da moeda, onde a nossa tarefa é calcular o total de maneiras possíveis de produzir a soma x usando as moedas. Por exemplo, se as moedas forem coins = {1,3,4} e x = 5, existem um total de 6 maneiras:
                
                • 1+1+1+1+1             • 3+1+1
                • 1+1+3                 • 1+4
                • 1+3+1                 • 4+1

                Novamente, podemos resolver esse problema de forma recursiva. Vamos denotar solve(x) o número de caminhos para formar a soma x. Por exemplo, se as moedas forem coins={1,3,4}, então sole(5) = 6 e a fórmula recursiva É
                
                solve(x) = solve(x-1)+solve(x-1)+solve(x-4).
                
                Em seguida, o caso geral da função recursiva é o seguinte:

                (fórmula pag 79 pdf)

                Se x<0, o valor é 0, porque não existem soluções. Se x=0, o valor é 1, porque existe apenas uma maneira de formar uma soma vazia. Caso contrário, calculamos a soma de todos os valores da forma solve(x - c), onde c está em coins.
                O código a seguir constrói um array count de modo que count[x] seja igual ao valor de solve(x) para 0 <= x <= n:

                count[0] = 1;
                for(int x = 1; x <= n; x++){
                    for(auto c : coins){
                        if(x-c >= 0){
                            count[x] += count[x-c];
                        }
                    }
                }

                Frequentemente o número de soluções é tão grande que não é necessário calcular o exato número, mas é suficiente oferecer como resposta o mudulo m onde, por exemplo, m = 10^9 + 7. Isso pode ser feito alterando o código para que todos os cálculos sejam feitos módulo m. No código acima, é suficiente adiciona a linha:
                
                count[x] %= m;

                após a linha
                
                count[x] += count[x-c];

                Agora já discutimos todas as ideias básica da programação dinâmica. Já que a programação dinâmica pode ser usada em muitas situações diferentes, agora passaremos por um conjunto de problemas que mostram mais exemplos sobre as possibilidades da programação dinâmica.

        ->Subsequência crescente mais longa:
            Nosso primeiro problema é encontrar a mais longa subsequência crescente em um array de n elementos. Isso é, uma sequência máxima de elementos do array que vai da esquerda para direita, e cada elemento da sequência é maior que o elemento anterior. Por exemplo, no array

            (imagem 1 pg 80 pdf)
            
            a subsequência crescente mais longa contém 4 elementos:

            (imagem 2 pg 80 pdf)

            Deixe length(k) denotar o comprimento da subsequência crescente mais longa que termina na posição k. Assim, se calculamos todos os valor de length(k) onde 0 <= k <= n-1, encontraremos o tamanho da maior subsequência crescente. Por exemplo, os valores da função do array acima é a seguinte:

            length(0) = 0;
            length(1) = 1;
            length(2) = 2;
            length(3) = 1;
            length(4) = 3;
            length(5) = 2;
            length(6) = 4;
            length(7) = 2;
            
            Por exemplo, length(6) = 4, porque a maior subsequência crescente que termina na posição 6 consiste em 4 elementos.
            Para calcular o valor de length(k), devemos encontrar uma posição i < k para qual a array[i] < array[k] e length(i) seja o maior possível. Então, sabemos que length(k) = length(i)+1, porque essa é a maneira ideal de adicionar array[k] a uma subsequência. No entanto, se não houver tal posição i, então length(k) = 1, o que significa que a subsequência contém apenas array[k].
            Uma vez que todos os valores possam ser calculados a partir de seus valores menores, podemos usar a programação dinâmica. No código a seguir, os valores da função serão armazenados em um array chamado length.

            for(int k = 0; k < n; k++){
                length[k] = 1;
                for(int i = 0; i < k; i++){
                    if(array[i] < array[k]){
                        length[k] = max(length[k], length[i]+1);
                    }
                }
            }

            Esse código trabalha em tempo O(n²), porque consiste em 2 laços aninhados. No entanto, também é possível implementar o cálculo de programação dinâmica de maneira mais eficiente em tempo O(nlogn). Você consegue encontrar uma maneira de fazer isso?

        ->Caminhos em um gride:
            Nosso próximo problema é encontrar um caminho do canto superior esquerdo para o canto inferior direito em uma grade n x n, de forma que só nos movemos para baixo e para direita. Cada quadrado contém um inteiro positivo, e o caminho deve ser construído de forma que a soma dos valores ao longo do caminho seja a maior possível.
            A seguinte imagem mostra um ótimo caminho para em um gride:

            (imagem 1 pag 81 pdf)

            A soma dos valores no caminho é 67, e é a maior soma possível de ser feita em um caminho do canto superior esquerdo para o canto inferior direito.
            Assuma que as linhas e colunas do gride são numeradas de 1 até n, e value[y][x] é igual ao valor do quadrado (y,x). Deixaremos sum(y,x) denotar o máximo da soma em um caminho do quadrado (y,x) do canto superior esquerdo. Agora, sum(n,n) nos diz a soma máximo do canto superior esquerdo até o canto inferior direito. Por exemplo, no gride acima, sum(5,5) = 67.
            Podemos calcular as somas recursivamente da seguinte forma:

            sum(y,x) = max(sum(y,x-1), sum(y-1,x)) + value[y][x]

            A fórmula recursiva é baseada na observação que o caminho termina no quadrado (y,x) pode vir ou do quadrado(y,x-1) ou do quadrado (y-1,x):

            (imagem 1 pg 82 pdf)

            Assim, selecionamos a direção que irá maximizar a soma. Assumimos que sum(y,x) = 0 se y = 0 ou x = 0 (porque não existem tais caminhos), então a fórmula recursiva também funciona quando y = 1 ou x = 1.
            Como a função sum tem dois parâmetros, o array de programação dinâmica também terá duas dimensões. Por exemplo, podemos usar um array 
            
            int sum[N][N];

            e calcular a forma da seguinte forma:

            for(int y = 1; y <= n; y++){
                for(int x = 1; x <= n; x++){
                    sum[y][x] = max(sum[y][x-1], sum[y-1][x]) + value[y][x];
                }
            }

            A complexidade de tempo do algoritmo é O(n²).

        ->Problema da mochila:
            O termo "knapsack" refere-se a problemas nos quais um conjunto de objetos é dado, e subconjuntos com certas propriedades precisam ser encontrados. Problemas de mochila muitas vezes podem ser resolvidos usando programação dinâmica.
            Nesta seção, focamos no seguinte problema: Dada uma lista de pesos [w1, w2, ..., wn], determine todas as somas que podem ser construídas usando os pesos. Por exemplo, se os pesos forem [1, 3, 3, 5], as seguintes somas são possíveis:
            
            (imagem 2 pg 82 pdf)

            Neste caso, todas as somas entre 0 e 12 são possíveis, exceto 2 e 10. Por exemplo, a soma 7 é possível porque podemos selecionar os pesos [1, 3, 3].
            Para resolver o problema, focamos em subproblemas onde usamos apenas os primeiros k pesos para construir somas. Seja possible(x, k) = true se pudermos construir uma soma x usando os primeiros k pesos e, caso contrário, possible(x, k) = false. Os valores da função podem ser calculados de forma recursiva da seguinte maneira:

            possible(x,k) = possible(x−wk,k −1)∨possible(x,k −1)

            A fórmula é baseada no fato de que podemos ou não usar o peso wk na soma. Se usarmos wk, a tarefa restante é formar a soma x - wk usando os primeiros k - 1 pesos, e se não usarmos wk, a tarefa restante é formar a soma x usando os primeiros k - 1 pesos. Como casos base,

            (fórmula 1 pg 83 pdf)

            pois se nenhum peso for usado, só podemos formar a soma 0.
            A tabela a seguir mostra todos os valores da função para os pesos [1,3,3,5] (o símbolo "X" indica os valores verdadeiros):

            (table 1 pg 83 pdf)

            Após calcular esses valores, possible(x,n) nos diz se podemos construir uma soma x usando todos os pesos.
            Vamos considerar que W é a soma total dos pesos. A seguinte solução de programação dinâmica em O(nW) corresponde à função recursiva:
            
            possible[0][0] = true;
            for(int k = 1; k <=n; k++){
                for(int x = 0; x <= W; x++){
                    if(x-w[k] >= 0) possible[x][k] |= possible[x-w[k]][k-1];
                    possible[x][k] |= possible[x][k-1];
                }
            }

            No entanto, aqui está uma implementação melhor que usa apenas um array unidimensional "possible[x]" para indicar se podemos construir um subconjunto com a soma x. O truque é atualizar o array da direita para a esquerda para cada novo peso:

            possible[0] = true;
            for (int k = 1; k <= n; k++) {
                for (int x = W; x >= 0; x--) {
                    if (possible[x]) possible[x+w[k]] = true;
                }
            }

            Note que a ideia geral apresentada aqui pode ser usada em muitos problemas de mochila (knapsack). Por exemplo, se nos forem dados objetos com pesos e valores, podemos determinar para cada soma de pesos o valor máximo da soma de um subconjunto.

        ->Distância de edição:
            A distância de edição, também conhecida como distância de Levenshtein, é o número mínimo de operações de edição necessárias para transformar uma string em outra As operações de edição permitidas são as seguintes:

            • inserir um caractere (por exemplo, ABC → ABCA)
            • remover um caractere (por exemplo, ABC → AC)
            • modificar um caractere (por exemplo, ABC → ADC)

            Por exemplo, a distância de edição entre LOVE e MOVIE é 2, porque primeiro podemos performar a operação LOVE -> MOVE (modificação) e então a operação MOVE->MOVIE (inserção). Esse é o menor número possível de operação, porque é claro que uma operação não seria o suficiente.
            Supondo que é nos dada uma string x de tamanho n e uma string y de tamanho m, e nós queremos calcular a distância de edição entre x e y. Para resolver esse problem, definimos a função distance(a,b) que dá a distância de edição entre os prefixos x[0...a] e y[0...b]. Portanto, usando essa função, a distância de edição entre x e y é igual distance(n-1,m-1).
            Podemos calcular os valores de distance da seguinte forma:

            distance(a,b) = min(distance(a,b-1)+1, distance(a-1,b)+1, distance(a-1,b-1)+cost(a,b)).

            Aqui, cost(a,b) = 0 se x[a] = y[b], e caso contrário cost(a,b) = 1. A fórmula considera as seguintes maneiras de editar a string x:

            • distance(a, b-1): inserir um caractere no final de x
            • distance(a-1,b): remover o último caractere de x
            • distance(a-1,b-1): corresponder ou modificar o último caractere de x

            Nos dois primeiros casos, é necessária uma operação de edição (inserir ou remover). No último caso, se x[a] = y[b], podemos corresponder os últimos caracteres sem edição, caso contrário, é necessária uma operação de edição (modificar).

            (imagem 1 pg 84 pdf)

            O canto inferior direito da tabela nos dis que a distância de edição entre LOVE  e MOVIE é 2. A tabela também mostra como construir o a sequência mais curta das operação de edição. Nesse caso, o caminho é o seguinte:

            (imagem 1 pg 85 pdf)

            
            Os últimos caracteres de LOVE e MOVIE são iguais, então a distância de edição entre eles é igual à distância de edição entre LOV e MOVI. Podemos usar uma operação de edição para remover o caractere I de MOVI. Portanto, a distância de edição é um número maior do que a distância de edição entre LOV e MOV, e assim por diante.

        ->Contagem de telhas:
            Às vezes, os estados de uma solução de programação dinâmica são mais complexos do que combinações fixas de números. Como exemplo, considere o problema de calcular o número de maneiras distintas de preencher uma grade n × m usando telhas de tamanho 1 × 2 e 2 × 1. Por exemplo, uma solução válida para a grade 4 × 7 é:

            (imagem 2 pg 85 pdf)

            e o número total de soluções é 781.
            O problema pode ser resolvido usando programação dinâmica percorrendo a grade linha por linha. Cada linha em uma solução pode ser representada como uma string que contém m caracteres do conjunto (conjunto apresentado na página 85 pdf). Por exemplo, a solução acima consiste em quatro linhas que correspondem às seguintes strings:

            (strings pag 85 pdf)

            Deixe count(k, x) representar o número de maneiras de construir uma solução para as linhas 1...k da grade de modo que a string x corresponda à linha k. É possível usar programação dinâmica aqui, porque o estado de uma linha é limitado apenas pelo estado da linha anterior.

            (finalizar tradução pelo word, devido símbolos estranhos).
        
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 8: