#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Capítulo 1:

Entrada e saída:

    Os seguintes comandos devem ser colocados no início do código para um melhor desempenho de entrada e saída:
    ios::sync_with_stdio(0);
    cin.tie(0);

    "\n" é mais rápido do que o endl, porque o endl sempre causa uma operação de limpeza (flush).

    Os comandos scanf e printf do C funcionam no C++, são mais rápidos, mas também mais complicados de se usar.

    Código para conseguir pegar uma linha inteira separada por espaços:
    string s;
    getline(cin, s);

    Se a quantidade de entradas for desconhecida, pode-se usar:
    while (cin >> x) {
    // code
    }

    Algumas competições utilizam arquivos, deixarei o código que deve ser colocado no início do código para esses casos:
    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Números flutuantes:

    Para números flutuantes, temos dois tipos de variáveis em C++, double (64-bit) e long double (80-bit)

    Pode ser arriscado comparar números flutuantes devido os erros de precisão, então pode ser melhor assumir que dois números são iguais se a subtração entre eles for menor que um número muito pequeno, como 10^-9. Como exemplo:
    if (abs(a-b) < 1e-9) {
    // a and b are equal
    }

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Dicas para diminuir o código:

    Usar typedef para diminuir o nome de variáveis, por exemplo:
    typedef long long ll; typedef vector<int> vi; typedef pair<int,int> pi;

->Macros:

    Outra maneira de diminuir os códigos é definir macros. Um macro significa que certas linhas no código serão alteradas antes da compilação. Exemplos:
    #define F first
    #define S second
    #define PB push_back()
    #define MP make_pair()

    Um macro também pode ter parâmetros, o que faz ser possível diminuir loops e outras estruturas. Exemplo:
    #define REP(i,a,b) for (int i = a; i <= b; i++)

    Esse código:
    for (int i = 1; i <= n; i++) {
    search(i);
    }

    Vira isso:
    REP(i,1,n) {
    search(i);
    }

    Algumas vezes, macros causam bugs que são difíceis de detectar. Por exemplo, considere o código abaixo que calcula o quadrado de um número:
    #define SQ(a) a*a

    Esse código:
    cout << SQ(3+3) << "\n";

    Corresponde a isso:
    cout << 3+3*3+3 << "\n"; // 15

    Uma melhor versão desse macro seria:
    #define SQ(a) (a)*(a)

    Agora, o código:
    cout << SQ(3+3) << "\n";

    Corresponde a:
    cout << (3+3)*(3+3) << "\n" // 36

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Matemática:

->Soma:
    PA: n(a+b)/2 onde a é o primeiro número, b é o último número e n a quantidade de números da sequência
    PG: bk-a/k-1 onde b é o último número, a é o primeiro número e k é a razão entre os números

->Teoria dos conjuntos:
    Um conjunto é uma coleção de elementos, como: X = {5, 8, 9, 10}

    |S| denota o tamanho do conjunto, ou seja a quantidade de elementos dentro do conjunto.

    Por exemplo, |X| = 4

    Se S possui um elemento x, dizemos que x ∈ s, e caso não, x ∉ S.
    5 ∈ X e 7 ∉ X.

    Novos conjuntos podem ser criados com as seguintes operações:
        ->Interseção; União; Complemento; Diferença.

Se cada elemento do conjunto A também pertence ao conjunto S, dizemos que A é um subconjunto de S.Alguns conjuntos frequentemente usados são N (números naturais), Z (números inteiros), Q (números racionais) e R (números reais).

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Lógica:
    ->O valor de uma expressão lógica é verdadeiro (1) ou falso (0). Os operadores lógicos mais importantes são: ¬ (negação), ∧ (conjunção), ∨ (disjunção), ⇒ (implicação) e ⇔ (equivalência).
    ->Predicado é uma expressão que é verdadeira ou falsa dependendo dos parâmetros. Os predicados geralmente são representados por letras maiúsculas. Por exemplo, podemos definir um predicado P(x) que é verdadeiro exatamente quando x é um número primo. Usando essa definição, P(7) é verdadeiro, mas P(8) é falso.
    ->Um quantificador conecta uma expressão lógica aos elementos de um conjunto. Os quantificadores mais importantes são ∀ (para todo) e ∃ (existe).

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 2:

->Complexidade de tempo: 
    A eficiência dos algoritmos é muito importante em uma competição de programação. É fácil implementar uma solução que resolve os problemas devagar, mas o verdadeiro desafio está em implementar algoritmos rápidos.
    A complexidade de tempo estima quanto tempo um algoritmo vai usar para cada entrada. A ideia é representar a eficiência de uma função que tem como parâmetro o tamanho da entrada. Calculando a complexidade de tempo, podemos descobrir se o algoritmo é rápido o suficiente para ser implementado.

    Regras do cálculo:
        A complexidade de tempo dos algoritmos é representada por O(...) em que o 3 pontos representam alguma função. Geralmente, a variável n denota o tamanho da entrada. Por exemplo, se a entrada for um array de números, n será o tamanho do array, e se a entrada for uma string, n será o comprimento da string.

        Laços:
            Uma razão comum do motivo de um algoritmo ser lento é por conter muitos laços que dependem do tamanho de entrada. Quanto mais loops aninhados o algoritmo contém, mais lento ele se torna. Se houver k loops aninhados, a complexidade de tempo é O(n^k).
            Por exemplo, a complexidade do código abaixo é O(n):
                for(int i = 0; i <= n; ++i){
                    //code
                }
            E esse código possui complexidade de O(n²):
                for(int i = 0; i < n; ++i){
                    for(int j = 0; j < n; ++j){
                        // code
                    }
                }
        
        Ordem de grandeza:
            A complexidade de tempo não diz exatamente a quantidade de vezes que o código dentro do laço será executado, apenas mostra a ordem de grandeza. No exemplo a seguir, o código dentro do loop será executado 3n, n+5 e ~n/2 vezes, mas a complexidade de cada código é O(n).
            for(int i = 0; i < 3*n; ++i){
                //code
            }

            for(int i = 0; i < n+5; ++i){
                //code
            }

            for(int i = 0; i < n; i+=2){
                //code
            }

        Fases:
            Se um algoritmo consiste em consecutivas fases, o tempo total de complexidade será o maior tempo de complexidade de uma única fase. A razão disso é que a fase mais lenta será o gargalo do código.
            Para exemplo, o código seguinte consiste em 3 fases que tem como complexidade de tempo, respectivamente, O(n), O(n²) e O(n). Então, a complexidade total será O(n²)!
            for(int i = 0; i < n; ++i){
                //code
            }

            for(int i = 0; i < n; ++i){
                for(int j = 0; j < n; ++j){
                    //code
                }
            }

            for(int i = 0; i < n; ++i){
                //code
            }

        Várias variáveis:
            De vez em quando, a complexidade de tempo depende de vários fatores. Nesse caso, a fórmula da complexidade de tempo conterá várias variáveis.
            Para exemplo, a complexidade do código abaixo é O(n*m):
                for(int i = 0; i < n; ++i){
                    for(int j = 0; j < m; ++j){
                        //code
                    }
                }

        Recursividade:
            A complexidade de tempo de uma função recursiva depende do número de vezes que a função é chamada e da complexidade de tempo de uma única chamada. A complexidade de tempo total é o produto desses valores.
            Para exemplo, considere o código abaixo:
                void f (int n){
                    if(n==1) return;
                    f(n-1);
                }
                
            A chamada f(n) causa n chamadas de função, e a complexidade de cada chamada é O(1). Então, a complexidade de tempo será O(n).
            
            Como outro exemplo, temos a seguinte função:
                void g(int n){
                    if(n == 1) return;
                    g(n-1);
                    g(n-1);
                }

            Nesse caso, cada chamada de função gera outras duas chamadas, exceto para n=1. Vamos ver o que acontece quando g é chamado com um parâmetro n. O seguinte esquema nos mostra a chamada de função produzida por uma chamada:

            Chamada de função  | Número de chamadas
            g(n)                                  1
            g(n-1)                                2
            g(n-2)                                4
            ...                                 ...
            g(1)                                2^n-1

            Baseado nisso, a complexidade de tempo será:
                1+2+4+...+2^n-1 = 2^n - 1 = O(2^n).

        Classes de complexidade de tempo:

            O(1) - Complexidade constante: O tempo de execução do algoritmo não depende do tamanho da entrada; é sempre constante;
            O(log n) - Complexidade logarítmica: O tempo de execução do algoritmo cresce de forma logarítmica em relação ao tamanho da entrada;
            O(square(n)) - Um algoritmo de raiz quadrada é mais lento que O(log n) mas mais rápido que O(n). 
            O(n) - Complexidade linear: O tempo de execução do algoritmo cresce linearmente em relação ao tamanho da entrada;
            O(n²) - Complexidade quadrática: O tempo de execução do algoritmo cresce quadraticamente em relação ao tamanho da entrada;
            O(n³) - Complexidade cúbica: O tempo de execução do algoritmo cresce cubicamente em relação ao tamanho da entrada;
            O(nlogn) - Complexidade log-linear: O tempo de execução do algoritmo cresce de forma log-linear em relação ao tamanho da entrada.
            O(2^n) - Essa complexidade de tempo frequentemente indica que o algoritmo itera por todos os subconjuntos dos elementos de entrada. Por exemplo, os subconjuntos de {1,2,3} são vazios, {1}, {2}, {3}, {1,2}, {1,3}, {2,3} e {1,2,3}.
            O(n!) - Essa complexidade de tempo frequentemente indica que o algoritmo itera por todas as permutações dos elementos de entrada. Por exemplo, as permutações de {1,2,3} são (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2) e (3,2,1).

            Um algoritmo é polinomial se sua complexidade de tempo for no máximo O(n^k), onde k é uma constante. Todas as complexidades de tempo acima, exceto O(2^n) e O(n!), são polinomiais. Na prática, a constante k geralmente é pequena, e, portanto, uma complexidade de tempo polinomial significa aproximadamente que o algoritmo é eficiente. 
            A maioria dos algoritmos neste livro é polinomial. Ainda assim, existem muitos problemas importantes para os quais nenhum algoritmo polinomial é conhecido, ou seja, ninguém sabe como resolvê-los de forma eficiente. Problemas NP-difíceis são um conjunto importante de problemas para os quais nenhum algoritmo polinomial é conhecido.
            
        Estimando eficiência:

            Calculando a complexidade de tempo de um algoritmo, é possível checar, antes da implementação, se ele é eficiente o suficiente para resolver o problema. O ponto de partida para estimativas é o fato que os computadores modernos podem executar algumas centenas de milhões de números de operação em segundos.
            Para exemplo, vamos assumir que o tempo limite para um problema seja de 1 segundo e o tamanho de entrada seja n = 10^5. Se a complexidade de tempo for O(n²), o algoritmo iria performar cerca de (10^5)² = 10^10 operações. Isso deve demandar um pouco mais que 10 segundos, então o algoritmo parece ser lento demais para resolver o problema.
            Por outro lado, dado o tamanho de entrada, nós podemos tentar adivinhar o tempo de complexidade que o algoritmo deve ter para resolver o problema. A tabela a seguir contém algumas estimativas de tempo assumindo o tempo limite de 1 segundo.
            
            Tamanho da entrada | Tempo de complexidade requirido
            n <= 10                                        O(n!)
            n <= 20                                        O(2^n)
            n <= 500                                       O(n³)
            n <= 5000                                      O(n²)
            n <= 10^6                         O(nlogn) or O(n)
            n é maior                           O(1) ou O(log n)

            Para exemplo, se o tamanho da entrada for n = 10^5, é provável que a complexidade de tempo esperado seja de O(n) ou O(log n). Essa informação faz com que seja mais fácil projetar o algoritmo, pois descarta abordagens que resultariam em um algoritmo com uma complexidade de tempo pior.
            Mesmo assim, é importante lembrar que o tempo de complexidade é apenas uma estimativa da eficiência, pois pode esconder fatores constantes. Por exemplo, um algoritmo que rode em tempo O(n) pode performar n/2 ou 5n operações. 

        Soma máxima de um subconjunto:

            Frequentemente existem diversos algoritmos para resolver um problema, de forma que suas complexidades de tempo sejam diferentes. Esse seção discute um problema clássico que possui uma solução direta em O(n³). Porém, projetando um melhor algoritmo, é possível resolver o problema em O(n²) ou até mesmo em O(n).
            
            Problema: dado um array de n números, nossa missão é calcular a soma máxima de um subconjunto, ou seja, a maior soma possível de uma sequência de valores consecutivos em um array. O problema se torna interessante quando podem haver valores negativos no array. Por exemplo, no array:
            [-1, 2, 4, -3, 5, 2, -5, 2]

            O subarray seguinte produz uma soma máxima de 10:
            [-1, 2, 4, -3, 5, 2, -5, 2]
            2+4+(-3)+5+2 = 10

            Assumimos que um array vazio é permitido, então a soma máxima de um subarray sempre será, pelo menos, 0.

            Algoritmo 1:
                Uma solução direta para resolver o problema é percorrer todos os subarray possíveis, calcular a soma dos valores em cada subarray e manter a soma máxima. O seguinte código implementa esse algoritmo:

                int best = 0;
                for(int a = 0; a < n; a++){
                    for(int b = a; b < n; b++){
                        int sum = 0;
                        for(int k = a; k <= b; k++){
                            sum += array[k];
                        }
                        best = max(best, sum);
                    }
                }
                cout << best << "\n";

                As variáveis a e b fixam o primeiro e o último index do subarray, e a soma dos valores é calculada e armazenada na variável sum. A variável best contém a soma máximo encontrada durante a procura.
                A complexidade de tempo desse código é O(n³), pois consiste em 3 array aninhados que dependem do tamanho de entrada.

            Algoritmo 2:
                É facíl tornar o algoritmo 1 mais eficiente removendo um laço dele. Isso é possível calculando a soma ao mesmo tempo em que a extremidade direita do subarray se move. O resultado é o código abaixo:
                
                int best = 0;
                for(int a  = 0; a <n; a++){
                    int sum = 0;
                    for(int b = a; b < n; b++){
                        sum+= array[b];
                        best = max(best, sum);
                    }
                }
                cout << best << "\n";

            Algoritmo 3:
                Surpreendentemente, é possível resolver esse problema em um tempo O(n), o que significa que um único laço é o suficiente. A ideia é calcular, para cada uma das posições do array, a soma máxima de um subarrray que termina nessa posição. Após isso, a resposta para o problema será o máximo entre essas somas.
                Considere o sub problema de encontrar a soma máxima de um subarray que termina na posição k. Existem 2 possibilidades:
                1 - O subarray apenas contém o elemento na posição k
                2 - O subarray consiste em um subarray que termina na posição k -1, seguido pelo elemento na posição k.
                No segundo caso, uma vez que queremos encontrar um subarray com soma máxima, o subarray que termina na posição k-1 deve ter a soma máxima. Assim, podemos resolver o problema de forma eficiente calculando a soma máxima do subarray para cada posição final da esquerda para a direita.
                O seguinte código implementa esse algoritmo:

                int best = 0, sum = 0;
                for(int k = 0; k < n; k++){
                    sum = max(array[k], sum+array[k]);
                    best = max(best, sum);
                }
                cout << best << "\n";

                O algoritmo contém apenas um laço que depende do tamanho da entrada, então a complexidade de tempo é O(n). Esse é o melhor tempo possível, pois qualquer outro algoritmo para o problema examina cada elemento do array pelo menos uma vez.
            
            Comparação de eficiência:
                É interessante estudar o quão eficiente são os algoritmos na prática. A seguinte tabela mostra o tempo de execução os algoritmos acima para diferentes valores de n em um computador moderno.
                Para cada teste, a entrada foi gerada aleatoriamente. O tempo necessário para a leitura da entrada não foi mensurado.
                
                array de tamanho n  | Algoritmo 1 | Algoritmo 2 | Algoritmo 3
                                10²          0.0s          0.0s         0.0s
                                10³          0.1s          0.0s         0.0s
                                10^4      > 10.0s          0.0s         0.0s
                                10^5      > 10.0s          0.1s         0.0s
                                10^6      > 10.0s          5.3s         0.0s
                                10^7      > 10.0s       > 10.0s         0.0s
                                10^8      > 10.0s       > 10.0s         0.0s

                A comparação mostra que os algoritmos são eficientes quando o tamanho de entrada é pequeno, mas entradas maiores trazem a tona a diferença entre o tempo de execução dos algoritmos. Algoritmo 1 se torna mais lento quando n = 10^4, e o Algoritmo 2 se torna mais lento quando n = 10^5. Apenas o algoritmo 3 é hábil à processar até mesmo as maiores entradas instantaneamente.
            
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 3:

->Ordenação:
    Ordenação é um problema fundamental de design de algoritmo. Vários algoritmos eficientes usam ordenação como uma sub-rotina, porque é frequentemente mais fácil processar dados se os elementos estiverem ordenados.
    Por exemplo, o problema "um array possui dois elementos iguais?" é fácil de resolver usando ordenação. Se o array possuir dois elementos iguais, eles estarão do lado um do outro depois da ordenação, então é fácil encontrá-los. Também, o problema "qual o elemento mais frequente no array?" pode ser resolvido de forma similar.
    Existem vários algoritmos para ordenação, e eles são bons exemplos de como aplicar diferentes técnicas de design de algoritmos. Os algoritmos gerais de ordenação eficientes atuam em tempo O(nlogn), e vários algoritmos que usam ordenação como sub-rotina também tem essa complexidade de tempo.

    Teoria da ordenação:
        O problema básico na ordenação é o seguinte:
        Dado um array que contém n elementos, sua tarefa é ordenar os elementos em ordem crescente.

        Por exemplo, o array:
        [1, 3, 8, 2, 9, 2, 5, 6]

        Deve estar assim após a ordenação:
        [1, 2, 2, 3, 5, 6, 8, 9]

    Algoritmos em O(n²):
        Algoritmos simples de ordenação de array funcionam em tempo O(n²). Esses algoritmos são curtos e frequentemente consistem em 2 laços aninhados. Um famoso algoritmo de ordenação com tempo O(n²) é o bubble sort, no qual os elementos "borbulham" no array de acordo com seus valores.
        O bubble sort consiste em n fases. Em cada fase, o algoritmo percorre os elementos do array. Sempre que 2 elementos consecutivos não estiverem na ordem correta, o algoritmo troca eles de posição. O algoritmo pode ser implementado com o código abaixo:

        for(int i = 0; i < n; ++i){
            for(int j = 0; j < n-1; ++j){
                if(array[j] > array[j+1]){
                    swap(array[j], array[j+1]);
                }
            }
        }

        Após a primeira fase do algoritmo, o maior valor estará na posição correta, e geralmente, após k fases, os k maiores elementos estarão nas posições corretas. Assim, após n fases, todo o array está ordenado.

        Por exemplo, no array:
        [1, 3, 8, 2, 9, 2, 5, 6]

        a primeira fase do bubble sort mudará os seguintes da seguinte forma:
        [1, 3, 2, 8, 9, 2, 5, 6]

        [1, 3, 2, 8, 2, 9, 5, 6]

        [1, 3, 2, 8, 2, 5, 9, 6]

        [1, 3, 2, 8, 2, 5, 6, 9]

    Inversões:
        Bubble sort é um exemplo de um algoritmo de ordenação que sempre troca elementos consecutivos no array. Isso torna o tempo de complexidade de um algoritmo como esse sempre ser, pelo menos, O(n²), porque no pior caso, O(n²) trocas são requeridas para ordenar o array.
        Um conceito útil ao analisar algoritmos de ordenação é a inversão: um par de elementos do array {array[a], array[b]} tal que a < b and array[a] > array[b], ou seja, esses elementos estão na ordem errada. Por exemplo, o array:
        [1, 2, 2, 6, 3, 5, 9, 8]

        possui 3 inversões: (6, 3), (6, 5) e (9, 8). O número de inversões indica o quanto de trabalho se terá para ordenar um array. Um array está completamente ordenado quando não há inversões. Por outro lado, se os elementos do array estão em ordem inversa, o número de inversões é o maior possível: 

        1 + 2 + ... + (n-1) = (n(n-1))/2 = O(n²)

        Trocar um par de elementos consecutivos que estão na ordem errada remove exatamente uma inversão do array. Portanto, se o algoritmo de ordenação só pode trocar elementos consecutivos, cada troca remove pelo menos uma inversão, e o tempo de complexidade do algoritmo se torna ao menos O(n²).

    Algoritmo em tempo O(nlogn):
        É possível ordenar um array de maneira eficiente em tempo O(nlogn) usando algoritmos que não se limitam à troca de elementos consecutivos. Um algoritmo desse tipo é o merge sort, qual se baseia em recursividade.
        Merge sort ordena um subarray array[a...b] da seguinte forma:
        1. Se a = b, não faça nada, porque o subarray já está ordenado.
        2. Calcule a posição do elemento do meio: k = ~[a+b/2].
        3. Recursivamente ordene o subarray array[a...k].
        4. Recursivamente ordene o subarray array[k+1...b].
        5. Junte os subarrays ordenados array[a...k] e array[k+1...b] em um subarray ordenado array[a...b].

        O merge sort é um algoritmo eficiente, pois reduz pela metade o tamanho do subarray em cada etapa. A recursão consiste em O(log n) níveis, e processar cada um dos níveis leva tempo O(n). Juntar os subarrays array[a...k] e array[k+1...b] é possível de ser feito em tempo linear O(1), porque eles já estão ordenado.
        Por exemplo, considere ordenar o seguinte array:
        [1, 3, 6, 2, 8, 2, 5, 9]
        
        O array será divido em 2 subarrayas da seguinte forma:
        [1, 3, 6, 2] [8, 2, 5, 9]

        Então, os subarrays serão ordenados recursivamente da seguinte forma:
        [1, 2, 3, 6] [2, 5, 8, 9]

        Finalmente, o algoritmo junta os dois subarrays ordenados e cria um último array ordenado:
        [1, 2, 2, 3, 5, 6, 8, 9]

    Limite inferior de ordenação:
        É possível ordenar um array mais rápido do que em tempo O(nlogn)? Acontece que isso não é possível quando nos restringimos a algoritmos de ordenação baseados em comparar os elementos do array.
        O limite inferior para a complexidade de tempo pode ser provado considerando a ordenação como um processo em que cada comparação de dois elementos fornece mais informações sobre o conteúdo do array. O processo cria a seguinte árvore:

                x < y?
            x<y?       x<y?
        x<y?   x<y? x<y?  x<y?

        Aqui, "x<y?" significa que vários elementos x e y estão sendo comparados. Se x<y, o processo continua para a esquerda, caso contrário, para a direita. O resultado do processo são os possíveis caminhos para se ordenar o array, um total de n! formas. Por essa razão, a altura da árvore deve ser no mínimo
            log2(n!) = log2(1) + log2(2) + ... + log2(n)
        Obtemos um limite inferior para essa soma escolhendo os últimos n/2 elementos e alterando o valor de cada elemento para log2(n/2). Isso nos fornece uma estimativa
            log2(n!) >= (n/2) * log2(n/2)
        então a altura da árvore e o mínimo de etas possíveis para um algoritmos de ordenação no pior caso é nlogn.

    Counting sort:
        O limite inferior de nlogn não se aplica a algoritmos que não comparam elementos do array, mas usam outras informações. Como exemplo temos o algoritmo counting sort, que ordena um array em tempo O(n) assumindo que cada um dos elementos do array é um inteiro entre 0...c e c = O(n).
        O algoritmo cria um array de registro, cujos índices são elementos do array original. O algoritmo percorre o array original e calcula quantas vezes cada elemento aparece no array.
        Por exemplo, o array:
        [1, 3, 6, 9, 9, 3, 5, 9]

        corresponde ao seguinte array de registro:
         1  2  3  4  5  6  7  8  9
        [1, 0, 2, 0, 1, 1, 0, 0, 3]

        Por exemplo, o valor na posição 3 no array de registro é 2, pois o elemento 3 aparece 2 vezes no array original.
        A construção do array de registra leva tempo O(n). Depois disso, o array ordenado pode ser criado em tempo O(n) porque o número de ocorrências para cada elemento pode ser retirada do array de registro. Assim, o tempo de complexidade total do counting sort é O(n).
        Counting sort é um algoritmo eficiente, mas só pode ser usado quando a constante c é pequena o suficiente para que os elementos do array possam ser usados como índices no array de registro.

    Ordenando em C++:
        Quase sempre é uma má ideia usar algoritmos de ordenação feitos em casa em uma competição, pois existem boas implementações disponíveis nas linguagens de programação. Por exemplo, a biblioteca padrão do C++ contém a função sort que pode ser facilmente usada para ordenar arrays e outras estruturas de dados.
        Existem muitos benefícios em usar funções de bibliotecas. Primeiro, salva tempo porque não é necessário implementar uma outra função. Segundo, a implementação da biblioteca é com certeza correta e eficiente: não é provável que uma função de ordenar feita em casa seja melhor.
        Nessa seção, veremos como usar a função sort do C++. O seguinte código ordena um vetor em ordem crescente:

        vector<int> v = {4,2,5,3,5,8,3};
        sort(v.begin(), v.end());

        Após a ordenação, o conteúdo do vetor será [2,3,3,4,5,5,8]. O padrão de ordenação é crescente, mas a ordem inversa é possível com o código a seguir: 

        sort(v.rbegin(), v.rend());

        Um array comum pode ser ordenado da seguinte forma:
        
        int n = 7; //tamanho do array
        int a[] = {4,2,5,3,5,8,3};
        sort(a,a+n);

        O código seguinte ordena uma string s:

        string s = "monkey";
        sort(s.begin(), s.end());

        Ordenar uma string significa que os caracteres da string são ordenados. Por exemplo, a string "monkey" se torna "ekmnoy".

    Operadores de comparação:
        A função sort requer que um operador de comparação seja definido para o tipo de dados dos elementos a serem ordenados. Ao ordenar, esse operador será usado sempre que for necessário determinar a ordem de dois elementos.
        A maior parte dos tipos de dado do C++ possuem um operador de comparação embutido, então os elementos desse tipo podem ser ordenados automaticamente. Por exemplo, os números são ordenados de acordo com seus valores e strings são ordenadas em ordem alfabética.
        Pares(pair) são ordenados principalmente de acordo com seus primeiros elementos(first). No entanto, se os primeiros elementos de dois pares forem iguais, eles são ordenados de acordo com seus segundos elementos(second):
        
        vector<pair<int,int>> v;
        v.push_back({1, 5});
        v.push_back({2, 3});
        v.push_back({1, 2});
        sort(v.begin(), v.end());

        Após isso, a ordem dos pares será (1,2), (1,5) e (2,3).
        
        De uma maneira similar, tuplas (tuple) são ordenadas primeiramente pelo seu primeiro elemento, secundariamente pelo segundo elemento, etc.:
        
        vector<tuple<int,int,int>> v;
        v.push_back({2,1,4});
        v.push_back({1,5,3});
        v.push_back({2,1,3});
        sort(v.begin(), v.end());

        Após isso, a ordem das tuplas será (1,5,3),(2,1,3) e (2,1,4).

    Estruturas definidas pelo usuário:
        Estruturas definidas pelo usuário não possuem um operado de comparação automaticamente. O operador deve ser definido dentro do struct como uma função operator<, cujo parâmetro é outro elemento do mesmo tipo. O operador deve retornar true se o elemento for menor que o parâmetro, e falso se for maior.
        Por exemplo, a seguinte estrutura P contém as coordenadas x e y de um ponto. O operador de comparação é definido então os pontos são ordenados primeiramente pela coordenada x e secundariamente pela coordenada y.

        struct P{
            int x, y;
            bool operator<(const P & p){
                if (x != p.x) return x < p.x;
                else return y < p.y;
            }
        };

    Funções de comparação:
        Também é possível oferecer uma função de comparação externa para função sorte como uma função de retorno de chamada (callback function). Por exemplo, a seguinte função de comparação comp ordena strings primeiramente pelo tamanho e secundariamente pela ordem alfabética:

        bool comp(string a, string b) {
            if(a.size() != b.size()) return a.size() < b.size();
            return a < b;
        }

        Agora, um vetor de strings pode ser ordenado dessa forma:

        sort(v.begin(), v.end(), comp);

    Busca binária:
        Um método padrão para procurar um elemento em um array é usar um laço for que percorre os elementos do array. Por exemplo, o seguinte código procura se um elemento x está no array:

        for(int i = 0; i < n; ++i){
            if(x == array[i]){
                // x encontrado no index i
            }
        }

        A complexidade de tempo dessa abordagem é O(n), porque no pior caso é necessário checar todos os elementos do array. Se a ordem dos elementos for arbitrária, então essa é a melhor abordagem possível, pois não há informações adicionais disponíveis sobre onde no array devemos procurar pelo elemento x.
        Por outro lado, se o array estiver ordenado, a situação é diferente. Nesse caso, é possível performar a procura de maneira muito mais rápida, porque oa ordem dos elementos no array guia a procura. Os seguintes algoritmos de busca binária procuram eficientemente por um elemento em um array ordenado em um tempo de O(logn).

        ->Método 1:
            A maneira usual de se implementar a busca binária se assemelha a procurar uma palavra em um dicionário. A procura mantém uma região ativa no array, que inicialmente contém todos os elementos do array. Em seguida, um certo número de passos é realizado, cada um dos quais reduz pela metade o tamanho da região.
            Em cada etapa, a busca verifica o elemento do meio da região ativa. Se o elemento do meio for o elemento procurado, então a busca termina. Caso contrário, a busca continua recursivamente na metade esquerda ou direita da região, dependendo do valor do elemento do meio.
            A ideia acima pode ser implementada da seguinte maneira:

            int a  = 0, b = n-1;
            while(a <= b){
                int k = (a+b)/2;
                if(array[k] == x){
                    //x encontrado no índice k
                }
                if(array[k] > b) b = k-1;
                else a = k+1;
            }
            
            Nessa implementação, a região ativa é a...b, e inicialmente a região é 0...n-1. O algoritmo reduz pela metade o tamanho da região a cada etapa, então a complexidade de tempo é O(logn).

        ->Método 2:
            Um método alternativo de implementa a busca binário é baseado em uma maneira eficiente de percorrer os elementos do array. A ideia é fazer saltos e diminuir a velocidade quando se está mais perto do elemento-alvo.
            A busca percorre o array da esquerda para a direita, e o comprimento inicial do salto é n/2. Em cada etapa, o comprimento do salto será dividido pela metade: primeiro n/4, depois n/8, n/16, etc., até que finalmente o comprimento seja 1. Após os saltos, ou o elemento-alvo foi encontrado, ou sabemos que ele não aparece no array.
            O seguinte código implementa a ideia acima:

            int k = 0;
            for(int b = n/2; b >= 1; b/=2){
                while(k+b < n && array[k+b]<=x) k+=b;
            }
            if(array[k] == x){
                // x encontrado no índice k
            }

            Durante a busca, a variável b contém o tamanho do salto atual. A complexidade de tempo do algoritmo é O(logn), pois o código no laço while é executado no máximo duas vezes para cada comprimento de salto.

    Funções do C++:
        A biblioteca padrão do C++ contém as seguintes funções que são baseadas em busca binária e funcionam em tempo logarítmico:
        ->lower_bound retorna um ponteiro para o primeiro elemento cujo valor é pelo menos x.
        ->upper_bound retorna um ponteiro para o primeiro elemento cujo valor seja maior que x.
        ->equal_range retorna os dois ponteiros acima.

        As funções assumem que o array está ordenado. Se não houver tal elemento, o ponteiro aponta para o elemento após o último elemento do array. Por exemplo, o seguinte código determina se um array contém um elemento com valor x:

        auto k = lower_bound(array, array+n, x) - array;
        if(k < n && array[k]==x){
            //x encontrado no índice k
        }
        
        Então, o código a seguir conta o número de elementos cujo valor é x:
        
        auto a = lower_bound(array, array+n, x);
        auto b = upper_bound(array, array+n, x);
        cout << b-a << "\n";

        Usando equal_range, o código se torna menor:

        auto r = equal_range(array, array+n, x);
        cout << r.second-r.first << "\n";

    Encontrando a menor solução:
        Um importante uso da busca binária é encontrar a posição onde o valor de uma função muda. Suponha que desejamos encontrar o menor valor k que é uma solução válida para um problema. Nós temos uma função "ok(x)" que retorna verdadeiro se x for uma solução válida e falso caso contrário. Além disso, sabemos que "ok(x)" é falso quando x < k e verdadeiro quando x >= k. A situação é a seguinte:

        x     | 0       1   ...   k-1      k    k+1   ...
        ok(x) |false false  ... false   true   true   ...

        Agora, o valor de k pode ser encontrado usando busca binária:

        int x = -1;
        for(int b = z; b >= 1; b/=2){
            while (!ok(x+b)) x+=b;
        }
        int k = x+1;

        A busca encontra o maior valor de x para o qual ok(x) é falso. Portanto, o próximo valor k = x+1 é o menor valor possível para o qual ok(k) é verdadeiro. O tamanho inicial do salto, z, deve ser suficientemente grande, por exemplo, algum valor para o qual sabemos de antemão que ok(z) é verdadeiro.
        O algoritmo chama a função ok O(log z) vezes, portanto, a complexidade de tempo total depende da função ok. Por exemplo, se a função funciona em tempo O(n), a complexidade de tempo total é O(n log z).

    Encontrando o valor máximo:
        Busca binária também pode ser usada para encontrar o valor máximo de uma função que primeiro aumenta e depois diminui. Nossa tarefa é encontrar uma posição k tal que:
        -> f(x) < f(x+1) quando x < k, e
        -> f(x) > f(x+1) quando x >= k.

        A ideia é usar busca binária para encontrar o maior valor de x para o qual f(x) < f(x+1). Isso implica que k = x=1, porque f(x+1) > f(x+2). O seguinte código implementa a busca:
        
        int x = -1;
        for(int b = z; b >= 1; b /= 2){
            while(f(x+b) < f(x+b+1)) x+= b;
        }
        int k = x+1;

        Observe que, ao contrário da busca binária comum, aqui não é permitido que valores consecutivos da função sejam iguais. Nesse caso, não seria possível saber como continuar a busca.
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 4:

->Estruturas de dados:
    Uma estrutura de dado é uma maneira de se armazenar dados na memória do computador. É importante escolher uma estrutura de dado apropriada para o problema, porque cada estrutura possui suas próprias vantagens e desvantagens. A questão crucial é: quais operações são eficientes na estrutura de dado escolhida?
    Esse capítulo introduz as mais importantes estruturadas de dados que estão na biblioteca padrão do C++. É uma boa ideia usar a biblioteca padrão sempre que possível, porque isso salva tempo. Mais tarde nesse livro, aprenderemos mais sobre estruturas de dados mais sofisticadas que não estão disponíveis na biblioteca padrão.

    Array dinâmicos:
        Um array dinâmico é um array qual pode ter seu tamanho alterado durante a execução do programa. O array dinâmico mais popular em C++ é a estrutura vector, qual pode ser usada praticamente como um array qualquer.
        O seguinte código cria um vector vazio e adiciona 3 elementos nele:

        vector<int> vi;
        vi.push_back(3); //[3]
        vi.push_back(2); //[3,2]
        vi.push_back(5); //[3,2,5]

        Após isso, os elementos podem ser acessados como um array comum:

        cout << v[0] << "\n"; //3
        cout << v[1] << "\n"; //2
        cout << v[2] << "\n"; //5

        A função size returna a quantidade de elementos em um vector. O seguinte código percorre o vector e imprime todos os elementos nele:

        for(int i = 0; i < vi.size(); i++){
            cout << v[i] << "\n";
        }

        Uma maneira mais fácil de percorrer o array segue no código abaixo:

        for(auto x : vi) {
            cout << x << "\n";
        }

        A função back retorna o último elemento no vector, e a função pop_back remove o último elemento.

        vector<int> vi;
        v.push_back(5);
        v.push_back(2);
        cout << v.back() << "\n"; //2
        v.pop_back();
        cout << v.back() << "\n"; //5

        O seguinte código cria um vetor com 5 elementos:

        vector<int> v = {2,4,2,5,1};

        Outra maneira de criar um vetor é dar o número de elemento e o valor inicial para cada elemento:

        //tamanho 10, valor inicial 0
        vector<int> vi(10);

        //tamanho 10, valor inicial 5
        vector<int> vi(10, 5);

        A implementação interna de um vector utiliza um array comum. Se o tamanho de um vector aumentar e o array se tornar pequeno demais, um novo array com mais memória alocada é criado e todos os elementos são movidos para o novo array. No entanto, isso não acontece com frequência e a complexidade de tempo média do push_back é O(1).
        A estrutura de string também é um array dinâmico que pode ser usado quase como um vector. Além disso, há uma sintaxe especial para as strings que não está disponível para as outras estruturas de dados. Strings podem ser combinadas usando o símbolo +. A função substr(k,x) retorna a substring que começa em k e possui tamanho x, e a função find(t) encontra a posição da primeira ocorrência da substring t.
        O seguinte código apresenta algumas operações com strings:

        string a  = "hatti";
        string b = a+a;
        cout << b << "\n"; //hattihatti
        b[5] = 'v';
        cout << b << "\n"; //hattivatti
        string c = b.substr(3,4);
        cout << c << "\n"; //tiva

    Estruturas Set:
        "Set" é uma estrutura de dado que mantém coleções de elementos. As operações básicas de sets são inserção de elemento, busca de elemento e remoção de elemento.
        A biblioteca padrão do C++ contém 2 implementações da estrutura "set": A estrutura "set" baseada em uma árvore binária balanceada e suas operação funcionam em tempo O(log n). A estrutura "unordered_set" usa hashing, e suas operações funcionam em tempo O(1) em média.
        A escolha de qual implementação de "set" usar muitas vezes é uma questão de gosto. O benefício da estrutura "set" é que ela mantém a ordem dos elementos e fornece funções que não estão disponíveis na "unordered_set". Por outro lado, "unordered_set" pode ser mais eficiente.
        O seguinte código cria um "set" que contém inteiros, e mostra algumas operações. A função insert adiciona um elemento ao "set", a função count retorna o número de ocorrências de um elemento no "set" e a função erase remove um elemento do "set".

        set<int> s;
        s.insert(3);
        s.inset(2);
        s.insert(5);
        cout << s.count(3) << "\n"; //1
        cout << s.count(4) << "\n"; //0
        s.erase(3);
        s.insert(4);
        cout << s.count(3) << "\n"; //0
        cout << s.count(4) << "\n" //1

        Um set pode ser usado como um vetor, mas não é possível acessar os elementos usando a notação []. O seguinte código cria um "set", imprime o número de elementos nele e percorre todos os elementos.

        set<int> s = {2,5,6,8};
        cout << s.size() << "\n"; //4
        for(auto x : s){
            cout << x << "\n";
        }

        Uma importante propriedade dos "sets" é que todos os elementos deles são distintos. Assim, a função count sempre retorna 0 (o elemento não está no "set") ou 1 (o elemento está no "set"), e a função insert nunca adiciona um elemento caso ele já esteja no "set". O seguinte código ilustra isso:

        set<int> s;
        s.insert(5);
        s.insert(5);
        s.insert(5);
        cout << s.count(5) << "\n"; //1

        C++ também contém as estruturas "multiset" e "unordered_multiset" que funcionam de forma semelhante a "set" e "unordered_set", mas podem conter múltiplas instâncias de um mesmo elemento. Por exemplo, no seguinte código todas as 3 instâncias do número 5 são adicionadas ao "multiset":

        multiset<int> s;
        s.insert(5);
        s.insert(5);
        s.insert(5);
        cout << s.count(5) << "\n"; //3

        A função erase remove todas as instâncias do elemento de um "multiset":
        s.erase(5);
        cout << s.count(5) << "\n"; //0

        Muitas vezes, apenas uma instância pode ser removida, qual pode ser feito com o código abaixo:
        
        s.erase(s.find(5));
        cout << s.count(5) << "\n"; //2

    Estruturas Map:
        Uma estrutura "map" é um array generalizado que consiste em pares chave-valor. Enquanto as chaves em um array comum são sempre inteiros consecutivos 0,1,...,n-1, onde n é o tamanho do array, as chaves no "map" podem ser de qualquer tipo de dado e não precisam ter valores consecutivos.
        A biblioteca padrão do C++ contém 2 implementações para o "map" que correspondem as implementações do "set": a estrutura "map" baseada em uma árvore binária balanceada e que o acesso de elementos leva tempo O(log n), enquanto a estrutura "unordered_map" usa hashing e o acesso dos elemento leva tempo O(1) em média.
        O seguinte código cria um "map" one as chaves são strings e os valores são inteiros:

        map<string,int> m;
        m["monkey"] = 4;
        m["banana"] = 3;
        m["harpsichord"] = 9;
        cout << m["banana"] << "\n"; // 3

        Se o valor da chave for requerido e o "map" não o conte, a chave é automaticamente adicionada ao "map" com o valor padrão. Por exemplo, no seguinte código, a chave "aybabtu" com valor 0 é adicionada ao "map":

        map<string,int> m;
        cout << m[aybabtu] << "\n"; // 0

        A função count checa se existe uma chave no "map":

        if(m.count(aybabtu)){
            //chave existe
        }

        O seguinte código imprime todas as chaves e valores do "map":
        
        for(auto x : m){
            cout << x.first << " " << x.second << "\n";
        }

    Iteradores e intervalos:
        Muitas funções da biblioteca padrão do C++ operam com iteradores. Um iterador é uma variável que aponta para um elemento em uma estrutura de dado.
        Os iteradores frequentemente usados begin e end definem o intervalo que contém todos os elementos de uma estrutura de dados. O iterador begin aponta para o primeiro elemento na estrutura de dados, e o iterador end aponta para a posição após o último elemento. A situação é a seguinte: 

        { 3, 4, 6, 8, 12, 13, 14, 17 }
          |                          |
        s.begin()                   s.end()

        Note a assimetria nos iteradores: s.begin() aponta para um elemento na estrutura de dados, enquanto s.end() aponta para fora da estrutura de dados. Portanto, o intervalo definido pelos iteradores é semi-aberto.

        ->Trabalhando com intervalos:
            Iterados usados em funções da biblioteca padrão do C++ que recebem um intervalo de elementos em uma estrutura de dados. Normalmente, queremos processar todos os elementos em uma estrutura de dados, então os iterados begin e end são fornecidos para a função.
            Por exemplo, o seguinte código ordena um vector usando a função sort, então inverte a ordem dos elementos usando a função reverse, e finalmente embaralha a ordem dos elementos usando a função random_shuffle.

            sort(v.begin(), v.end());
            reverse(v.begin(), v.end());
            random_shuffle(v.begin(), v.end());

            Essas funções também podem ser utilizadas em um array comum. Nesse caso, as funções recebem ponteiros para o array ao invés de iteradores:

            sort(a, a+n);
            reverse(a, a+n);
            random_shuffle(a, a+n);

        ->Iteradores de Set:
            Iteradores são frequentemente usados para acessar elemento de um "set". O seguinte código cria um iterador it que aponta para o menor elemento em um set:
            
            set<int>::iterator it = s.begin();

            Uma maneira menor de escrever o mesmo código é a seguinte:

            auto it = s.begin();

            O elemento para qual o iterador aponta pode ser acessado usando o símbolo *. Por exemplo, o seguinte código imprime o primeiro elemento de um "set":

            auto it = s.begin();
            cout << *it << "\n";

            Iteradores podem ser movidos usando os operado ++ (avançar) e -- (voltar), significando que o iterador se move para o próximo ou elemento anterior no "set". 
            O seguinte código imprime todos os elementos de um "set" em ordem crescente:

            for(auto it = s.begin(); it != s.end(); it++){
                cout << *it << "\n";
            }

            O seguinte código imprime o maior elemento em um "set":

            auto it = s.end(); it--;
            cout << *it << "\n";

            A função find(x) retorna um iterador que aponta para o elemento cujo valor é x. No entanto se "set" não possuir x, o iterador será igual ao end().

            auto it = s.find(x);
            if(it == s.end()){
                // x não foi encontrado
            }

            A função lower_bound(x) retorna um iterador que aponta para o menor elemento de um "set" cujo valor é pelo menos x, e a função upper_bound(x) retorna um iterador para o menor elemento no "set" cujo valor é maior que x. Em ambas as funções, se tal elemento não existir, o valor de retorno será end. Essas funções não existem na estrutura unordered_set qual não mantém a ordem dos elementos.
            Por exemplo, o seguinte código encontra o elemento mais próximo de x:

            auto it = s.lower_bound(x);
            if(it == s.begin()) {
                cout << *it << "\n";
            } else if(it == s.end()){
                it--;
                cout << *it << "\n";
            } else{
                int a = *it; it--;
                int b = *it;
                if(x-b < a-x) cout << b << "\n";
                else cout << a "\n";
            }

            O código assume que o "set" não está vazio, e percorre todos os possíveis casos usando o iterador it. Primeiro, o iterador aponta para o menor elemento cujo valor é pelo menos x. Se it for igual a begin, o elemento correspondente é o mais próximo de x. Se it for igual a end, o maior elemento no set é o mais próximo de x. Se nenhum dos casos anteriores for verdadeiro, o elemento mais próximo de x será o elemento que corresponde a it ou ao elemento anterior.

    Outras estruturas:
        ->Bitset:
            O "bitset" é um array cujo valor é 0 ou 1. Por exemplo, o seguinte código cria um bitset que contém 10 elementos:

            bitset<10> s;
            s[1] = 1;
            s[3] = 1;
            s[4] = 1;
            s[7] = 1;
            cout << s[4] << "\n"; //1
            cout << s[5] << "\n"; //0

            O benefício de usar bitsets é que precisa de menos memória que arrays comuns, porque cada elemento do bitset usa 1 bit de memória. Por exemplo, se n bits são armazenados em um array de inteiros, 32n bits de memória serão usados, mas um bitset correspondente só usa n bits de memória. Em adição, o valor de um bitset pode ser eficientemente manipulado usando operadores de bit, o que torna possível otimizar algoritmos usando conjuntos de bits.
            O seguinte código mostra outra maneira de criar o bitset acima:
            
            bitset<10> s(string("0010011010)); //da direita para a esquerda
            cout << s[4] << "\n"; //1
            cout << s[5] << "\n"; //0

            A função count retorna a quantidade de números 1 em um bitset:

            bitset<10> s(string("0010011010"));
            cout << s.count() << "\n"; //4

            O seguinte código mostra exemplos de uso de operadores de bits:
            
            bitset<10> a(string("0010110110"));
            bitset<10> b(string("1011011000"));
            cout << (a&b) << "\n"; // 0010010000
            cout << (a|b) << "\n"; // 101101110
            cout << (a^b) << "\n"; //1001101110

        ->Deque:
            Deque é um array dinâmico cujo tamanho pode ser eficientemente alterado em ambas extremidades do array. Assim como um vector, uma "deque" fornece as funções push_back e pop_back, mas também inclui as funções push_front e pop_front quais não estão disponíveis para um vector.
            Uma deque pode ser usada da seguinte forma:

            deque<int> d;
            d.push_back(5); // [5]
            d.push_back(2); // [5, 2]
            d.push_front(3); // [3, 5, 2]
            d.pop_back();   // [3, 5]
            d.pop_front(); //[5]

            A implementação interna de uma deque é mais complexa que a deu um vector, e po essa razão, uma deque é mais lenta que um vector. Mesmo assim, adicionar ou remover elementos leva tempo O(1) em média para ambos os casos.

        ->Pilha:
            Uma pilha é uma estrutura de dado que fornece 2 operações em tempo O(1): adicionar um elemento ao topo e remover um elemento do topo. Isso só é acessar o elemento do topo de uma pilha.
            O seguinte código mostra como uma pilha pode ser usada:

            stack<int> s;
            s.push(3);
            s.push(2);
            s.push(5);
            cout << s.top(); //5
            s.pop();
            cout << s.top(); //5
            s.pop();
            cout << s.top(); //2

        ->Fila:
            Uma fila também fornece 2 operações em tempo O(1): adicionar um elemento ao final da fila e remover o primeiro elemento de uma fila. Só é possível acessar o primeiro e último elemento de uma fila.
            O seguinte código mostra como uma fila pode ser usada:

            queue<int> q;
            q.push(3);
            q.push(2);
            q.push(5);
            cout << q.front(); // 3
            q.pop();
            cout << q.front(); //2

        ->Fila de prioridade:
            A fila de prioridade sempre mantém um conjunto de elementos. As operações oferecidas são inserção e, dependendo do tipo de fila, busca e remoção do elemento mínimo ou máximo. Inserção e remoção levam tempo O(log n)m e busca leva tempo O(1).
            Embora um "set" ordenado suporta eficientemente todas as operações de uma fila de prioridade, o benefício de se usar uma fila de prioridade é que possui menos fatores constantes. Uma fila de prioridade é normalmente implementada usando um estrutura heap que é muito mais simples que um árvore binária balanceada usada no "set" ordenado.
            Por padrão, os elemento em uma fila de prioridade em C++ são ordenados em ordem decrescent, e é possível encontrar e remover o maior elemento na fila. O seguinte código ilustra isso:

            priority_queue<int> q;
            q.push(3);
            q,push(5);
            q.push(7);
            q.push(2);
            cout << q.top() << "\n'; // 7
            q.pop();
            cout << q.top() << "\n"; // 5
            q.pop();
            q.push(6);
            cout << q.top() << "\n"; // 6
            q.pop();

            Se quisermos criar uma fila de prioridade que pode encontrar e remover o menor elemento, podemos fazer da seguinte forma:

            priority_queue<int, vector<int>, greater<int>> q;

        ->Estruturas de dados baseadas em políticas:
            O compilador g++ também suporta estruturas de dados que não fazem parte da biblioteca padrão do C++. Tais estruturas são chamadas de estruturas de dados policy-based (baseadas em política). Para usar essas estruturas, as seguintes linhas precisam ser adicionadas ao código:
            
            #include <ext/pb_ds/assoc_container.hpp>
            using namespace __gnu_pbds;

            Após isso, podemos definir a estrutura de dados indexed_set que é como um "set", mas pode ser indexada como um array. A definição para um valor inteiro é a seguinte:

            typdef tree<int, null_type, less<int>, rb_tree_tag, tree_order_statistics_node_update> indexed_set;

            Agora podemos criar um "set" da seguinte forma:

            indexed_set s;
            s.insert(2);
            s.insert(3);
            s.insert(7);
            s.insert(9);

            A particularidade desse "set" é que temos acesso aos índices que os elementos teriam em um array ordenado. A função find_by_order retorna um iterador para o elemento em uma determinada posição:

            auto x = s.find_by_order(2);
            cout << *x << "\n"; //7

            E a função order_of_key retorna a posição de um elemento dado:
            
            cout << s.order_of_key(7) << "\n"; // 2

            Se o elemento não aparece no "set", então nós recebemos a posição que ele teria no "set":

            cout << s.order_of_key(6) << "\n"; //2
            cout << s.order_of_key(8) << "\n"; //3

            Ambas as funções trabalham em tempo logarítmico.

    Comparação com ordenação:
        É frequentemente possível resolver problemas usando tanto estruturas de dados quanto ordenação. Às vezes, existem diferenças notáveis na eficiência real dessas abordagens, que podem estar ocultas em suas complexidades de tempo.
        Vamos considerar um problema onde são dadas 2 listas A e B, ambas contendo n elementos. Nossa tarefa é calcular o número de elementos que pertencem a ambas as listas. Por exemplo, as listas

        A = [5,2,8,9,4] e B = [3,2,9,5],

        a resposta seria 3, porque os número 2, 5 e 9 pertencem a ambas as listas.
        Uma solução direta para o problema é percorrer todos os pares de elementos em tempo O(n²), mas a seguir vamos focar em algoritmos mais eficientes.

        ->Algoritmo 1:
            Construímos um conjunto de elementos que aparecem em A, e após isso, percorremos os elementos de B e checamos se cada elemento também pertence a A. Isso é eficiente porque os elemento de A estão em um conjunto. Usando a estrutura "set", a complexidade de tempo do algoritmos é O(nlogn).
        
        ->Algoritmo 2:
            Não é necessário manter o conjunto ordenado, então ao invés de usar a estrutura "set" podemos usar, também, a estrutura "unordered_set". Essa é uma maneira fácil de tornar o algoritmo mais eficiente, porque só precisamos alterar a estrutura de dados subjacente. A complexidade de tempo do novo algoritmo é O(n).

        ->Algoritmo 3:
            Ao invés de usar estruturas de dados, podemos usar ordenação. Primeiro, ordenamos ambas as listas A e B. Após isso, percorremos ambas as listas ao mesmo tempo e verificamos os elementos em comum. O tempo de complexidade da ordenação é O(nlogn), e o resto do algoritmo trabalha em tempo O(n), então o tempo total é O(nlog).

        ->Comparação de eficiência:
            A seguinte tabela mostra quão eficiente os algoritmos acima são quando n varia e os elementos das listas são inteiros aleatórios entre 1 e 10^9:

              n|  Algoritmo 1 | Algoritmo 2 | Algoritmo 3
           10^6|          1.5s|         0.3s|        0.2s
         2*10^6|          3.7s|         0.8s|        0.3s
         3*10^6|          5.7s|         1.3s|        0.5s
         4*10^6|          7.7s|         1.7s|        0.7s
         5*10^6|         10.0s|         2.3s|        0.9s

            Os algoritmos 1 e 2 são iguais exceto que usam estruturas de "set" diferentes. Nesse problemas, essa escolha tem um importante efeito no tempo de execução, porque o algoritmo 2 é 4-5 vezes mais rápido que o primeiro.
            De qualquer forma, o mais eficiente foi o algoritmo, qual usa ordenação. Ele apenas usa metade do tempo quando comparado com o algoritmo 2. Curiosamente, a complexidade tanto para o algoritmo 1 quanto para o algoritmo 3 é O(n log n), mas apesar disso, o algoritmo 3 é dez vezes mais rápido. Isso pode ser explicado pelo fato de que a ordenação é um procedimento simples e é feito apenas uma vez no início do algoritmo 3, e o restante do algoritmo funciona em tempo linear. Por outro lado, o algoritmo 1 mantém uma árvore binária balanceada complexa durante todo o algoritmo.

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Capítulo 5: